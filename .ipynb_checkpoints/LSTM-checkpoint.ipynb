{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9678b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of C:\\Users\\veena\\anaconda3\\lib\\site-packages\\vision-1.0.0-py3.9-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\veena\\anaconda3\\lib\\site.py\", line 169, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 562, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n",
      "C:\\Users\\veena\\anaconda3\\python.exe: No module named spacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob==0.15.3 in c:\\users\\veena\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\veena\\anaconda3\\lib\\site-packages (from textblob==0.15.3) (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\veena\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob==0.15.3) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\veena\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob==0.15.3) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\veena\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob==0.15.3) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\veena\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob==0.15.3) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\veena\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob==0.15.3) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of C:\\Users\\veena\\anaconda3\\lib\\site-packages\\vision-1.0.0-py3.9-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\veena\\anaconda3\\lib\\site.py\", line 169, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 562, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==2.2.3\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install textblob==0.15.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7234605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "#We are going to use word vectorization to embed the words\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('cleaned_tweets_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85f7f2e",
   "metadata": {},
   "source": [
    "# Preprocessing the tweets\n",
    "Here, we shall do things like change common short hands into full form (in order to deal with the issue of punctuation removal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34bd066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we convert words like can't, won't etc into cannot and will not\n",
    "short_form_dict = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",}\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b8da4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>uncleaned_text</th>\n",
       "      <th>account_type</th>\n",
       "      <th>emoji</th>\n",
       "      <th>emoji_text</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.874060e+17</td>\n",
       "      <td>We can't extract the\\n\"Avenge\" from\\n\"Scavenge...</td>\n",
       "      <td>bot</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>we can t extract the avenge from scavengers</td>\n",
       "      <td>https://t.co/FpAstfF67l</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.874060e+17</td>\n",
       "      <td>We can't get the\\n\"Other\" out of\\n\"Geothermal\"...</td>\n",
       "      <td>bot</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>we can t get the other out of geothermal</td>\n",
       "      <td>https://t.co/pw6xYLUph5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.874060e+17</td>\n",
       "      <td>You can't spell\\n\"Panoply\"\\nwithout \"Ply\" http...</td>\n",
       "      <td>bot</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>you can t spell panoply without ply</td>\n",
       "      <td>https://t.co/XYsPhTkcDX</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.874060e+17</td>\n",
       "      <td>Put the \"Myrrh\"\\nin \"Commercial\" https://t.co/...</td>\n",
       "      <td>bot</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>put the myrrh in commercial</td>\n",
       "      <td>https://t.co/yyadIxV48k</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.874060e+17</td>\n",
       "      <td>We need \"Dying\"\\nto spell \"Muddying\" https://t...</td>\n",
       "      <td>bot</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>we need dying to spell muddying</td>\n",
       "      <td>https://t.co/WJxZL1TkfM</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                     uncleaned_text  \\\n",
       "0  7.874060e+17  We can't extract the\\n\"Avenge\" from\\n\"Scavenge...   \n",
       "1  7.874060e+17  We can't get the\\n\"Other\" out of\\n\"Geothermal\"...   \n",
       "2  7.874060e+17  You can't spell\\n\"Panoply\"\\nwithout \"Ply\" http...   \n",
       "3  7.874060e+17  Put the \"Myrrh\"\\nin \"Commercial\" https://t.co/...   \n",
       "4  7.874060e+17  We need \"Dying\"\\nto spell \"Muddying\" https://t...   \n",
       "\n",
       "  account_type emoji emoji_text                                         text  \\\n",
       "0          bot    {}         []  we can t extract the avenge from scavengers   \n",
       "1          bot    {}         []     we can t get the other out of geothermal   \n",
       "2          bot    {}         []          you can t spell panoply without ply   \n",
       "3          bot    {}         []                  put the myrrh in commercial   \n",
       "4          bot    {}         []              we need dying to spell muddying   \n",
       "\n",
       "                       url tags  \n",
       "0  https://t.co/FpAstfF67l   []  \n",
       "1  https://t.co/pw6xYLUph5   []  \n",
       "2  https://t.co/XYsPhTkcDX   []  \n",
       "3  https://t.co/yyadIxV48k   []  \n",
       "4  https://t.co/WJxZL1TkfM   []  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3da18fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Things to keep: Capitalisation of words, punctuation, emojis and their positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e3b91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing steps (ie cleaning of text and including important information like emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0293fe5",
   "metadata": {},
   "source": [
    "# Train test split, model training and word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e9eff2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the X and y classes\n",
    "tweets['isBot'] = 0\n",
    "tweets.loc[tweets['account_type'] == 'bot', 'isBot'] = 1\n",
    "y = tweets.isBot\n",
    "X = tweets[['text']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83c6cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, just embed the cleaned tweets and see the results\n",
    "\n",
    "#store the cleaned tweets in a list of lists representing the split\n",
    "X_train_sentence = [d.split() for d in X_train.text.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "195ae116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the word to vector\n",
    "#try a 100 dimensional vector first\n",
    "dim = 100\n",
    "#Fit a Word2Vec model on our dataset\n",
    "w2v = gensim.models.Word2Vec(sentences = X_train_sentence, vector_size = dim, window = 10, min_count = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d4421ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('funny', 0.9556493759155273),\n",
       " ('damn', 0.9549897909164429),\n",
       " ('yeah', 0.9526556730270386),\n",
       " ('shit', 0.9496830701828003),\n",
       " ('lol', 0.9496800899505615),\n",
       " ('really', 0.9394170045852661),\n",
       " ('actually', 0.9369041919708252),\n",
       " ('yes', 0.9351949095726013),\n",
       " ('sad', 0.9337857961654663),\n",
       " ('okay', 0.9332491159439087)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1136d4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50966"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1af8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6cc7bdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'to': 2,\n",
       " 'a': 3,\n",
       " 'i': 4,\n",
       " 'and': 5,\n",
       " 'you': 6,\n",
       " 'of': 7,\n",
       " 'in': 8,\n",
       " 'for': 9,\n",
       " 'is': 10,\n",
       " 'it': 11,\n",
       " 's': 12,\n",
       " 'this': 13,\n",
       " 'on': 14,\n",
       " 'my': 15,\n",
       " 'that': 16,\n",
       " 'with': 17,\n",
       " 't': 18,\n",
       " 'be': 19,\n",
       " 'at': 20,\n",
       " 'your': 21,\n",
       " 'we': 22,\n",
       " 'are': 23,\n",
       " 'me': 24,\n",
       " 'have': 25,\n",
       " 'so': 26,\n",
       " 'all': 27,\n",
       " 'was': 28,\n",
       " 'just': 29,\n",
       " 'but': 30,\n",
       " 'can': 31,\n",
       " 'not': 32,\n",
       " 'from': 33,\n",
       " 'out': 34,\n",
       " 'what': 35,\n",
       " 'by': 36,\n",
       " 'one': 37,\n",
       " 'they': 38,\n",
       " 'm': 39,\n",
       " 'our': 40,\n",
       " 'as': 41,\n",
       " 'more': 42,\n",
       " 'if': 43,\n",
       " 'up': 44,\n",
       " 'get': 45,\n",
       " 'like': 46,\n",
       " 'an': 47,\n",
       " 'about': 48,\n",
       " 'now': 49,\n",
       " 'new': 50,\n",
       " 'he': 51,\n",
       " 'do': 52,\n",
       " 'will': 53,\n",
       " 'love': 54,\n",
       " 'time': 55,\n",
       " 'how': 56,\n",
       " 'when': 57,\n",
       " 'who': 58,\n",
       " 'there': 59,\n",
       " 'or': 60,\n",
       " 'no': 61,\n",
       " 'day': 62,\n",
       " 'has': 63,\n",
       " 'today': 64,\n",
       " 'here': 65,\n",
       " 're': 66,\n",
       " 'thank': 67,\n",
       " 'people': 68,\n",
       " 'don': 69,\n",
       " 'see': 70,\n",
       " 'been': 71,\n",
       " 'good': 72,\n",
       " 'us': 73,\n",
       " 'know': 74,\n",
       " 'his': 75,\n",
       " 'their': 76,\n",
       " 'back': 77,\n",
       " 'great': 78,\n",
       " 'would': 79,\n",
       " 'some': 80,\n",
       " 'her': 81,\n",
       " 'go': 82,\n",
       " 've': 83,\n",
       " 'them': 84,\n",
       " 'happy': 85,\n",
       " '2': 86,\n",
       " 'had': 87,\n",
       " 'she': 88,\n",
       " 'only': 89,\n",
       " 'need': 90,\n",
       " 'make': 91,\n",
       " 'got': 92,\n",
       " 'best': 93,\n",
       " '1': 94,\n",
       " 'much': 95,\n",
       " 'first': 96,\n",
       " 'way': 97,\n",
       " 'thanks': 98,\n",
       " 'right': 99,\n",
       " 'still': 100,\n",
       " 'year': 101,\n",
       " 'over': 102,\n",
       " 'think': 103,\n",
       " 'work': 104,\n",
       " 'life': 105,\n",
       " 'want': 106,\n",
       " 'why': 107,\n",
       " '3': 108,\n",
       " 'these': 109,\n",
       " 'too': 110,\n",
       " 'going': 111,\n",
       " 'week': 112,\n",
       " 'check': 113,\n",
       " 'll': 114,\n",
       " 'please': 115,\n",
       " 'last': 116,\n",
       " 'help': 117,\n",
       " 'years': 118,\n",
       " 'should': 119,\n",
       " 'show': 120,\n",
       " 'let': 121,\n",
       " 'did': 122,\n",
       " 'never': 123,\n",
       " 'via': 124,\n",
       " 'am': 125,\n",
       " 'very': 126,\n",
       " 'after': 127,\n",
       " 'really': 128,\n",
       " 'into': 129,\n",
       " 'were': 130,\n",
       " 'than': 131,\n",
       " 'him': 132,\n",
       " 'being': 133,\n",
       " 'off': 134,\n",
       " 'always': 135,\n",
       " 'game': 136,\n",
       " 'even': 137,\n",
       " 'because': 138,\n",
       " 'could': 139,\n",
       " 'well': 140,\n",
       " 'take': 141,\n",
       " 'come': 142,\n",
       " '5': 143,\n",
       " 'person': 144,\n",
       " 'live': 145,\n",
       " 'world': 146,\n",
       " 'any': 147,\n",
       " 'then': 148,\n",
       " 'look': 149,\n",
       " 'many': 150,\n",
       " 'also': 151,\n",
       " 'photo': 152,\n",
       " 'team': 153,\n",
       " 'everyone': 154,\n",
       " 'night': 155,\n",
       " 'where': 156,\n",
       " 'next': 157,\n",
       " 'say': 158,\n",
       " 'down': 159,\n",
       " 'home': 160,\n",
       " 'every': 161,\n",
       " 'most': 162,\n",
       " 'two': 163,\n",
       " 'd': 164,\n",
       " 'other': 165,\n",
       " 'family': 166,\n",
       " '10': 167,\n",
       " 'amazing': 168,\n",
       " 'free': 169,\n",
       " 'man': 170,\n",
       " 'watch': 171,\n",
       " 'ever': 172,\n",
       " 'better': 173,\n",
       " 'win': 174,\n",
       " 'may': 175,\n",
       " 'find': 176,\n",
       " 'made': 177,\n",
       " 'twitter': 178,\n",
       " '4': 179,\n",
       " 'again': 180,\n",
       " 'those': 181,\n",
       " 'join': 182,\n",
       " 'tonight': 183,\n",
       " 'hope': 184,\n",
       " 'big': 185,\n",
       " 'keep': 186,\n",
       " 'same': 187,\n",
       " 'support': 188,\n",
       " 'posted': 189,\n",
       " 'which': 190,\n",
       " 'before': 191,\n",
       " 'fun': 192,\n",
       " 'u': 193,\n",
       " 'someone': 194,\n",
       " 'since': 195,\n",
       " 'things': 196,\n",
       " 'feel': 197,\n",
       " 'days': 198,\n",
       " 'its': 199,\n",
       " 'said': 200,\n",
       " 'friends': 201,\n",
       " 'season': 202,\n",
       " 'use': 203,\n",
       " 'read': 204,\n",
       " 'video': 205,\n",
       " 'getting': 206,\n",
       " 'lol': 207,\n",
       " 'through': 208,\n",
       " 'doing': 209,\n",
       " 'yes': 210,\n",
       " 'long': 211,\n",
       " 'thing': 212,\n",
       " 'wait': 213,\n",
       " '2022': 214,\n",
       " 'old': 215,\n",
       " 'play': 216,\n",
       " 'stop': 217,\n",
       " 'something': 218,\n",
       " 'news': 219,\n",
       " 'looking': 220,\n",
       " 'sure': 221,\n",
       " 'birthday': 222,\n",
       " '6': 223,\n",
       " 'checked': 224,\n",
       " 'little': 225,\n",
       " 'another': 226,\n",
       " 'didn': 227,\n",
       " 'while': 228,\n",
       " 'book': 229,\n",
       " 'does': 230,\n",
       " 'around': 231,\n",
       " 'coming': 232,\n",
       " 'part': 233,\n",
       " 'money': 234,\n",
       " 'automatically': 235,\n",
       " 'real': 236,\n",
       " 'own': 237,\n",
       " 'october': 238,\n",
       " 'tomorrow': 239,\n",
       " 'music': 240,\n",
       " 'give': 241,\n",
       " '7': 242,\n",
       " 'such': 243,\n",
       " 'tell': 244,\n",
       " 'school': 245,\n",
       " 'favorite': 246,\n",
       " 'lot': 247,\n",
       " 'put': 248,\n",
       " 'learn': 249,\n",
       " 'beautiful': 250,\n",
       " 'link': 251,\n",
       " 'call': 252,\n",
       " 'miss': 253,\n",
       " 'bad': 254,\n",
       " 'share': 255,\n",
       " 'story': 256,\n",
       " 'away': 257,\n",
       " 'excited': 258,\n",
       " 'y': 259,\n",
       " '8': 260,\n",
       " 'start': 261,\n",
       " 'hey': 262,\n",
       " 'done': 263,\n",
       " 'full': 264,\n",
       " 'morning': 265,\n",
       " 'watching': 266,\n",
       " 'guys': 267,\n",
       " 'both': 268,\n",
       " 'oh': 269,\n",
       " 'working': 270,\n",
       " 'everything': 271,\n",
       " 'found': 272,\n",
       " 'trying': 273,\n",
       " 'friday': 274,\n",
       " 'women': 275,\n",
       " 'nothing': 276,\n",
       " 'shit': 277,\n",
       " 'anyone': 278,\n",
       " 'god': 279,\n",
       " 'without': 280,\n",
       " 'hard': 281,\n",
       " 'ready': 282,\n",
       " 'proud': 283,\n",
       " 'doesn': 284,\n",
       " 'might': 285,\n",
       " 'having': 286,\n",
       " 'weekend': 287,\n",
       " 'change': 288,\n",
       " 'few': 289,\n",
       " 'top': 290,\n",
       " 'followed': 291,\n",
       " 'city': 292,\n",
       " 'end': 293,\n",
       " 'special': 294,\n",
       " 'trump': 295,\n",
       " 'won': 296,\n",
       " 'actually': 297,\n",
       " 'episode': 298,\n",
       " 'house': 299,\n",
       " 'hear': 300,\n",
       " 'thought': 301,\n",
       " 'business': 302,\n",
       " 'friend': 303,\n",
       " 'yet': 304,\n",
       " 'vote': 305,\n",
       " 'soon': 306,\n",
       " 'talk': 307,\n",
       " 'care': 308,\n",
       " 'open': 309,\n",
       " 'believe': 310,\n",
       " 'month': 311,\n",
       " 'congratulations': 312,\n",
       " 'must': 313,\n",
       " 'finally': 314,\n",
       " 'making': 315,\n",
       " 'remember': 316,\n",
       " 'available': 317,\n",
       " 'follow': 318,\n",
       " 'together': 319,\n",
       " 'art': 320,\n",
       " 'place': 321,\n",
       " 'service': 322,\n",
       " 'maybe': 323,\n",
       " 'already': 324,\n",
       " 'movie': 325,\n",
       " 'country': 326,\n",
       " 'ago': 327,\n",
       " 'kids': 328,\n",
       " 'gonna': 329,\n",
       " 'says': 330,\n",
       " 'times': 331,\n",
       " 'account': 332,\n",
       " 'looks': 333,\n",
       " '9': 334,\n",
       " 'state': 335,\n",
       " 'high': 336,\n",
       " 'needs': 337,\n",
       " 'w': 338,\n",
       " 'try': 339,\n",
       " 'anything': 340,\n",
       " 'until': 341,\n",
       " 'media': 342,\n",
       " 'sorry': 343,\n",
       " 'black': 344,\n",
       " '30': 345,\n",
       " 'job': 346,\n",
       " 'point': 347,\n",
       " 'heart': 348,\n",
       " 'stay': 349,\n",
       " 'tickets': 350,\n",
       " 'health': 351,\n",
       " 'during': 352,\n",
       " 'enough': 353,\n",
       " 'hi': 354,\n",
       " 'name': 355,\n",
       " 'latest': 356,\n",
       " 'seen': 357,\n",
       " 'food': 358,\n",
       " 'online': 359,\n",
       " 'isn': 360,\n",
       " 'left': 361,\n",
       " 'community': 362,\n",
       " 'awesome': 363,\n",
       " 'sign': 364,\n",
       " 'went': 365,\n",
       " 'against': 366,\n",
       " 'listen': 367,\n",
       " 'true': 368,\n",
       " 'makes': 369,\n",
       " 'event': 370,\n",
       " 'used': 371,\n",
       " 'bring': 372,\n",
       " 'head': 373,\n",
       " 'each': 374,\n",
       " 'white': 375,\n",
       " 'wish': 376,\n",
       " 'nice': 377,\n",
       " 'guy': 378,\n",
       " 'baby': 379,\n",
       " 'party': 380,\n",
       " 'loved': 381,\n",
       " 'cool': 382,\n",
       " 'wrong': 383,\n",
       " 'mean': 384,\n",
       " 'wow': 385,\n",
       " 'post': 386,\n",
       " 'forward': 387,\n",
       " '100': 388,\n",
       " 'run': 389,\n",
       " 'chance': 390,\n",
       " 'fuck': 391,\n",
       " 'power': 392,\n",
       " 'lost': 393,\n",
       " 'social': 394,\n",
       " 'able': 395,\n",
       " 'fall': 396,\n",
       " 'tweet': 397,\n",
       " 'president': 398,\n",
       " 'far': 399,\n",
       " 'set': 400,\n",
       " 'enjoy': 401,\n",
       " 'future': 402,\n",
       " 'super': 403,\n",
       " '20': 404,\n",
       " 'fans': 405,\n",
       " 'song': 406,\n",
       " 'whole': 407,\n",
       " 'different': 408,\n",
       " 'experience': 409,\n",
       " 'else': 410,\n",
       " 'yeah': 411,\n",
       " 'under': 412,\n",
       " 'using': 413,\n",
       " 'hours': 414,\n",
       " 'list': 415,\n",
       " 'series': 416,\n",
       " 'kind': 417,\n",
       " 'girl': 418,\n",
       " 'saying': 419,\n",
       " '12': 420,\n",
       " 'once': 421,\n",
       " 'buy': 422,\n",
       " 'children': 423,\n",
       " 'x': 424,\n",
       " 'early': 425,\n",
       " 'took': 426,\n",
       " 'pretty': 427,\n",
       " '0': 428,\n",
       " 'taking': 429,\n",
       " 'bio': 430,\n",
       " 'past': 431,\n",
       " 'app': 432,\n",
       " 'meet': 433,\n",
       " 'rest': 434,\n",
       " '11': 435,\n",
       " 'important': 436,\n",
       " 'football': 437,\n",
       " 'playing': 438,\n",
       " 'water': 439,\n",
       " 'order': 440,\n",
       " 'congrats': 441,\n",
       " 'yourself': 442,\n",
       " 'saw': 443,\n",
       " 'myself': 444,\n",
       " 'weeks': 445,\n",
       " 'least': 446,\n",
       " 'though': 447,\n",
       " 'fucking': 448,\n",
       " 'company': 449,\n",
       " '15': 450,\n",
       " 'late': 451,\n",
       " 'face': 452,\n",
       " 'came': 453,\n",
       " 'moment': 454,\n",
       " 'called': 455,\n",
       " 'unfollowed': 456,\n",
       " 'seeing': 457,\n",
       " 'games': 458,\n",
       " 'waiting': 459,\n",
       " 'line': 460,\n",
       " 'thinking': 461,\n",
       " 'hit': 462,\n",
       " 'haven': 463,\n",
       " 'visit': 464,\n",
       " 'others': 465,\n",
       " 'local': 466,\n",
       " 'public': 467,\n",
       " 'car': 468,\n",
       " 'pay': 469,\n",
       " 'sunday': 470,\n",
       " 'podcast': 471,\n",
       " 'three': 472,\n",
       " 'woman': 473,\n",
       " 'absolutely': 474,\n",
       " 'men': 475,\n",
       " 'wanted': 476,\n",
       " 'matter': 477,\n",
       " 'mom': 478,\n",
       " 'saturday': 479,\n",
       " 'mind': 480,\n",
       " 'second': 481,\n",
       " 'almost': 482,\n",
       " 'move': 483,\n",
       " 'students': 484,\n",
       " 'national': 485,\n",
       " 'talking': 486,\n",
       " 'told': 487,\n",
       " 'less': 488,\n",
       " 'definitely': 489,\n",
       " 'human': 490,\n",
       " 'summer': 491,\n",
       " 'hello': 492,\n",
       " 'covid': 493,\n",
       " 'gets': 494,\n",
       " 'incredible': 495,\n",
       " 'hot': 496,\n",
       " 'young': 497,\n",
       " 'peace': 498,\n",
       " 'giving': 499,\n",
       " 'office': 500,\n",
       " 'glad': 501,\n",
       " 'ok': 502,\n",
       " 'shows': 503,\n",
       " 'america': 504,\n",
       " 'between': 505,\n",
       " 'ask': 506,\n",
       " 'ass': 507,\n",
       " 'number': 508,\n",
       " 'crazy': 509,\n",
       " 'send': 510,\n",
       " 'phone': 511,\n",
       " 'heard': 512,\n",
       " 'lives': 513,\n",
       " 'words': 514,\n",
       " 'history': 515,\n",
       " 'red': 516,\n",
       " 'hate': 517,\n",
       " 'welcome': 518,\n",
       " 'wasn': 519,\n",
       " 'probably': 520,\n",
       " 'safe': 521,\n",
       " 'course': 522,\n",
       " 'guess': 523,\n",
       " 'living': 524,\n",
       " 'feeling': 525,\n",
       " 'perfect': 526,\n",
       " 'daily': 527,\n",
       " 'forget': 528,\n",
       " 'yesterday': 529,\n",
       " 'bit': 530,\n",
       " 'tour': 531,\n",
       " 'issue': 532,\n",
       " 'police': 533,\n",
       " 'single': 534,\n",
       " 'update': 535,\n",
       " '000': 536,\n",
       " 'biden': 537,\n",
       " 'easy': 538,\n",
       " 'months': 539,\n",
       " 'california': 540,\n",
       " 'level': 541,\n",
       " 'literally': 542,\n",
       " 'room': 543,\n",
       " 'comes': 544,\n",
       " 'wonderful': 545,\n",
       " 'fight': 546,\n",
       " 'idea': 547,\n",
       " 'wants': 548,\n",
       " 'fan': 549,\n",
       " 'click': 550,\n",
       " 'space': 551,\n",
       " 'album': 552,\n",
       " 'started': 553,\n",
       " 'behind': 554,\n",
       " 'film': 555,\n",
       " 'grateful': 556,\n",
       " 'side': 557,\n",
       " 'boy': 558,\n",
       " 'close': 559,\n",
       " '24': 560,\n",
       " 'light': 561,\n",
       " 'official': 562,\n",
       " 'break': 563,\n",
       " 'damn': 564,\n",
       " 'action': 565,\n",
       " 'e': 566,\n",
       " 'truly': 567,\n",
       " 'sale': 568,\n",
       " 'park': 569,\n",
       " '22': 570,\n",
       " 'star': 571,\n",
       " 'american': 572,\n",
       " 'b': 573,\n",
       " 'instead': 574,\n",
       " 'cause': 575,\n",
       " 'small': 576,\n",
       " 'n': 577,\n",
       " 'possible': 578,\n",
       " 'below': 579,\n",
       " 'project': 580,\n",
       " 'september': 581,\n",
       " 'goes': 582,\n",
       " 'hell': 583,\n",
       " 'sending': 584,\n",
       " '50': 585,\n",
       " 'forever': 586,\n",
       " 'website': 587,\n",
       " 'hold': 588,\n",
       " 'york': 589,\n",
       " 'details': 590,\n",
       " 'stuff': 591,\n",
       " 'case': 592,\n",
       " 'cannot': 593,\n",
       " '19': 594,\n",
       " 'huge': 595,\n",
       " 'sad': 596,\n",
       " 'christmas': 597,\n",
       " 'agree': 598,\n",
       " 'halloween': 599,\n",
       " 'body': 600,\n",
       " 'photos': 601,\n",
       " 'along': 602,\n",
       " 'across': 603,\n",
       " 'problem': 604,\n",
       " 'brother': 605,\n",
       " 'store': 606,\n",
       " 'reading': 607,\n",
       " 'access': 608,\n",
       " 'half': 609,\n",
       " 'running': 610,\n",
       " 'oct': 611,\n",
       " 'happen': 612,\n",
       " 'shot': 613,\n",
       " 'couldn': 614,\n",
       " 'war': 615,\n",
       " 'report': 616,\n",
       " 'youtube': 617,\n",
       " 'happened': 618,\n",
       " 'gotta': 619,\n",
       " 'save': 620,\n",
       " 'worth': 621,\n",
       " 'hour': 622,\n",
       " 'sometimes': 623,\n",
       " 'release': 624,\n",
       " 'books': 625,\n",
       " 'c': 626,\n",
       " 'law': 627,\n",
       " 'added': 628,\n",
       " 'entire': 629,\n",
       " 'class': 630,\n",
       " 'word': 631,\n",
       " 'exactly': 632,\n",
       " 'dm': 633,\n",
       " 'step': 634,\n",
       " 'sharing': 635,\n",
       " '18': 636,\n",
       " 'ad': 637,\n",
       " 'energy': 638,\n",
       " 'turn': 639,\n",
       " 'later': 640,\n",
       " 'fire': 641,\n",
       " 'group': 642,\n",
       " 'tv': 643,\n",
       " 'worst': 644,\n",
       " 'starting': 645,\n",
       " 'brand': 646,\n",
       " 'email': 647,\n",
       " 'rights': 648,\n",
       " 'road': 649,\n",
       " 'florida': 650,\n",
       " 'fact': 651,\n",
       " 'question': 652,\n",
       " 'shop': 653,\n",
       " 'vs': 654,\n",
       " 'answer': 655,\n",
       " 'celebrate': 656,\n",
       " 'dog': 657,\n",
       " 'opportunity': 658,\n",
       " 'means': 659,\n",
       " 'blue': 660,\n",
       " 'page': 661,\n",
       " 'funny': 662,\n",
       " 'self': 663,\n",
       " 'area': 664,\n",
       " 'dead': 665,\n",
       " 'walk': 666,\n",
       " 'luck': 667,\n",
       " 'wanna': 668,\n",
       " 'fantastic': 669,\n",
       " 'voice': 670,\n",
       " 'ya': 671,\n",
       " 'customer': 672,\n",
       " 'gone': 673,\n",
       " 'reason': 674,\n",
       " 'ones': 675,\n",
       " 'especially': 676,\n",
       " 'hair': 677,\n",
       " 'sent': 678,\n",
       " 'air': 679,\n",
       " 'giveaway': 680,\n",
       " 'market': 681,\n",
       " 'eat': 682,\n",
       " 'continue': 683,\n",
       " 'pre': 684,\n",
       " '14': 685,\n",
       " 'interested': 686,\n",
       " 'system': 687,\n",
       " 'questions': 688,\n",
       " 'final': 689,\n",
       " 'minutes': 690,\n",
       " 'missed': 691,\n",
       " 'center': 692,\n",
       " 'understand': 693,\n",
       " 'death': 694,\n",
       " 'child': 695,\n",
       " 'street': 696,\n",
       " 'girls': 697,\n",
       " 'west': 698,\n",
       " '21': 699,\n",
       " 'message': 700,\n",
       " 'needed': 701,\n",
       " '25': 702,\n",
       " 'million': 703,\n",
       " 'piece': 704,\n",
       " 'due': 705,\n",
       " 'collection': 706,\n",
       " 'club': 707,\n",
       " 'stream': 708,\n",
       " 'outside': 709,\n",
       " 'interview': 710,\n",
       " 'journey': 711,\n",
       " 'leave': 712,\n",
       " 'november': 713,\n",
       " 'enter': 714,\n",
       " 'thursday': 715,\n",
       " 'including': 716,\n",
       " 'newprofilepic': 717,\n",
       " 'create': 718,\n",
       " 'county': 719,\n",
       " 'court': 720,\n",
       " 'son': 721,\n",
       " 'hand': 722,\n",
       " 'government': 723,\n",
       " 'bro': 724,\n",
       " 'fast': 725,\n",
       " 'become': 726,\n",
       " 'strong': 727,\n",
       " 'dad': 728,\n",
       " 'building': 729,\n",
       " 'drop': 730,\n",
       " 'omg': 731,\n",
       " 'anniversary': 732,\n",
       " 'front': 733,\n",
       " 'im': 734,\n",
       " 'article': 735,\n",
       " 'loss': 736,\n",
       " 'code': 737,\n",
       " 'following': 738,\n",
       " 'yours': 739,\n",
       " 'played': 740,\n",
       " 'mine': 741,\n",
       " 'seems': 742,\n",
       " 'process': 743,\n",
       " 'tried': 744,\n",
       " 'plan': 745,\n",
       " 'pm': 746,\n",
       " 'deal': 747,\n",
       " 'paid': 748,\n",
       " 'short': 749,\n",
       " 'gave': 750,\n",
       " 'near': 751,\n",
       " 'f': 752,\n",
       " 'stories': 753,\n",
       " 'sounds': 754,\n",
       " 'texas': 755,\n",
       " 'meeting': 756,\n",
       " 'works': 757,\n",
       " 'rock': 758,\n",
       " 'campaign': 759,\n",
       " 'content': 760,\n",
       " 'north': 761,\n",
       " 'loving': 762,\n",
       " 'p': 763,\n",
       " 'date': 764,\n",
       " 'picture': 765,\n",
       " 'states': 766,\n",
       " 'data': 767,\n",
       " '1st': 768,\n",
       " 'dear': 769,\n",
       " 'honor': 770,\n",
       " 'often': 771,\n",
       " 'london': 772,\n",
       " 'act': 773,\n",
       " 'trip': 774,\n",
       " 'asked': 775,\n",
       " 'sweet': 776,\n",
       " '13': 777,\n",
       " 'appreciate': 778,\n",
       " 'card': 779,\n",
       " 'wearing': 780,\n",
       " 'alone': 781,\n",
       " 'lovely': 782,\n",
       " 'kid': 783,\n",
       " 'current': 784,\n",
       " 'cover': 785,\n",
       " 'style': 786,\n",
       " 'plus': 787,\n",
       " 'inside': 788,\n",
       " 'response': 789,\n",
       " 'pick': 790,\n",
       " 'lots': 791,\n",
       " 'race': 792,\n",
       " 'performance': 793,\n",
       " 'followers': 794,\n",
       " 'either': 795,\n",
       " 'fashion': 796,\n",
       " '99': 797,\n",
       " 'record': 798,\n",
       " 'ukraine': 799,\n",
       " 'snail': 800,\n",
       " 'workers': 801,\n",
       " 'cancer': 802,\n",
       " 'site': 803,\n",
       " 'add': 804,\n",
       " 'nyc': 805,\n",
       " 'r': 806,\n",
       " 'career': 807,\n",
       " 'okay': 808,\n",
       " 'calling': 809,\n",
       " 'stand': 810,\n",
       " 'learning': 811,\n",
       " 'thoughts': 812,\n",
       " '2020': 813,\n",
       " 'student': 814,\n",
       " 'price': 815,\n",
       " 'ain': 816,\n",
       " 'aren': 817,\n",
       " 'choose': 818,\n",
       " 'beat': 819,\n",
       " 'cat': 820,\n",
       " 'ways': 821,\n",
       " 'dude': 822,\n",
       " 'issues': 823,\n",
       " 'dream': 824,\n",
       " 'instagram': 825,\n",
       " 'watched': 826,\n",
       " 'beyond': 827,\n",
       " 'wouldn': 828,\n",
       " 'jobs': 829,\n",
       " 'beach': 830,\n",
       " 'currently': 831,\n",
       " 'catch': 832,\n",
       " 'takes': 833,\n",
       " 'gift': 834,\n",
       " 'truth': 835,\n",
       " 'based': 836,\n",
       " 'college': 837,\n",
       " 'box': 838,\n",
       " 'cute': 839,\n",
       " 'etc': 840,\n",
       " 'longer': 841,\n",
       " 'election': 842,\n",
       " 'travel': 843,\n",
       " 'mother': 844,\n",
       " 'south': 845,\n",
       " 'chat': 846,\n",
       " 'green': 847,\n",
       " '16': 848,\n",
       " 'india': 849,\n",
       " 'john': 850,\n",
       " 'couple': 851,\n",
       " 'opening': 852,\n",
       " 'success': 853,\n",
       " 'happening': 854,\n",
       " 'info': 855,\n",
       " '2018': 856,\n",
       " 'boys': 857,\n",
       " 'sound': 858,\n",
       " 'transponder': 859,\n",
       " 'trecru': 860,\n",
       " 'healthy': 861,\n",
       " 'mental': 862,\n",
       " 'taken': 863,\n",
       " 'drive': 864,\n",
       " 'felt': 865,\n",
       " 'low': 866,\n",
       " 'changed': 867,\n",
       " 'monday': 868,\n",
       " 'register': 869,\n",
       " 'winter': 870,\n",
       " 'biggest': 871,\n",
       " 'sense': 872,\n",
       " 'pass': 873,\n",
       " 'families': 874,\n",
       " 'feels': 875,\n",
       " 'happens': 876,\n",
       " 'apple': 877,\n",
       " 'design': 878,\n",
       " 'contact': 879,\n",
       " 'review': 880,\n",
       " 'asking': 881,\n",
       " 'eyes': 882,\n",
       " 'view': 883,\n",
       " 'leadership': 884,\n",
       " 'smile': 885,\n",
       " 'la': 886,\n",
       " 'queen': 887,\n",
       " 'folks': 888,\n",
       " 'fine': 889,\n",
       " 'o': 890,\n",
       " 'mr': 891,\n",
       " 'coffee': 892,\n",
       " 'weather': 893,\n",
       " 'wear': 894,\n",
       " '2023': 895,\n",
       " 'brought': 896,\n",
       " 'education': 897,\n",
       " 'artist': 898,\n",
       " 'program': 899,\n",
       " 'wishing': 900,\n",
       " 'parents': 901,\n",
       " '40': 902,\n",
       " 'imagine': 903,\n",
       " 'version': 904,\n",
       " 'san': 905,\n",
       " 'personal': 906,\n",
       " 'russia': 907,\n",
       " 'united': 908,\n",
       " 'international': 909,\n",
       " 'information': 910,\n",
       " 'dance': 911,\n",
       " 'listening': 912,\n",
       " 'wife': 913,\n",
       " 'speak': 914,\n",
       " '00': 915,\n",
       " 'quick': 916,\n",
       " 'met': 917,\n",
       " 'bought': 918,\n",
       " 'stupid': 919,\n",
       " '23': 920,\n",
       " 'dr': 921,\n",
       " 'worked': 922,\n",
       " 'celebrating': 923,\n",
       " 'four': 924,\n",
       " 'ahead': 925,\n",
       " 'hands': 926,\n",
       " 'moving': 927,\n",
       " 'prices': 928,\n",
       " 'jesus': 929,\n",
       " 'sir': 930,\n",
       " 'interesting': 931,\n",
       " 'tho': 932,\n",
       " 'deep': 933,\n",
       " 'cut': 934,\n",
       " 'lose': 935,\n",
       " 'return': 936,\n",
       " 'choice': 937,\n",
       " 'videos': 938,\n",
       " 'respect': 939,\n",
       " 'writing': 940,\n",
       " 'match': 941,\n",
       " 'role': 942,\n",
       " 'daughter': 943,\n",
       " 'complete': 944,\n",
       " 'ban': 945,\n",
       " 'breaking': 946,\n",
       " 'ride': 947,\n",
       " 'com': 948,\n",
       " 'middle': 949,\n",
       " 'teams': 950,\n",
       " 'grow': 951,\n",
       " 'sea': 952,\n",
       " 'gas': 953,\n",
       " 'training': 954,\n",
       " 'seriously': 955,\n",
       " 'gold': 956,\n",
       " 'rip': 957,\n",
       " 'test': 958,\n",
       " 'credit': 959,\n",
       " 'showing': 960,\n",
       " 'knew': 961,\n",
       " 'member': 962,\n",
       " 'poor': 963,\n",
       " 'movies': 964,\n",
       " 'wishes': 965,\n",
       " 'joined': 966,\n",
       " 'joy': 967,\n",
       " 'voting': 968,\n",
       " 'attention': 969,\n",
       " 'receive': 970,\n",
       " 'deserve': 971,\n",
       " 'los': 972,\n",
       " 'clear': 973,\n",
       " 'putting': 974,\n",
       " 'policy': 975,\n",
       " 'stage': 976,\n",
       " 'facebook': 977,\n",
       " 'members': 978,\n",
       " 'stars': 979,\n",
       " 'holiday': 980,\n",
       " 'character': 981,\n",
       " 'wild': 982,\n",
       " 'trailer': 983,\n",
       " 'track': 984,\n",
       " 'rt': 985,\n",
       " 'festival': 986,\n",
       " 'brilliant': 987,\n",
       " 'nfl': 988,\n",
       " 'former': 989,\n",
       " 'lil': 990,\n",
       " 'gives': 991,\n",
       " 'marketing': 992,\n",
       " 'tweets': 993,\n",
       " 'cash': 994,\n",
       " 'whatever': 995,\n",
       " 'missing': 996,\n",
       " 'build': 997,\n",
       " 'stock': 998,\n",
       " 'bill': 999,\n",
       " 'uk': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "82f645ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the words (this is for padding later)\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e139feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since twitter only allows 280 characters, we shall keep the max length to\n",
    "#the greater of 100 or whatever maximum length is found in the dataset\n",
    "max_length = max(100, max(len(x) for x in X_train_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a30bb919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we can pad the sequences\n",
    "X_train_tokens = pad_sequences(X_train_tokens, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6a4862c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42947, 100)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84d0b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "vocab = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c0dbc42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the embedding matrix according to our word embeddings\n",
    "embedding_mat = np.zeros((vocabulary_size, dim))\n",
    "num_words = 0\n",
    "#insert the word embeddings into our initialised object\n",
    "for word, token in tokenizer.word_index.items():\n",
    "\n",
    "    #get the corresponding vector to the word and the token\n",
    "    #if the word is present in the dictionary, then we append the corresponding matrix else we continue\n",
    "    if word in w2v.wv:\n",
    "        vector = w2v.wv[word]\n",
    "        embedding_mat[token] = vector\n",
    "        num_words +=1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7bc314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43846 57740\n"
     ]
    }
   ],
   "source": [
    "print(num_words, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3bd3d7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4922853 ,  1.7439225 ,  0.03323253, -0.06788983,  0.9944433 ,\n",
       "       -1.1523787 ,  0.2008604 ,  1.757623  , -1.1312804 , -1.5283166 ,\n",
       "        0.4049504 , -0.4245158 ,  1.0631258 , -0.87430376,  2.8444827 ,\n",
       "       -1.6614215 ,  3.3981173 , -2.7986977 , -1.2876256 , -0.83556664,\n",
       "        1.7448853 ,  1.4097966 ,  1.6122912 , -1.2898759 , -2.3379378 ,\n",
       "        1.6228114 , -1.1372364 , -0.90890193, -0.88947695,  0.36047032,\n",
       "       -0.32651895, -1.3694718 ,  0.31408098, -3.3128746 , -1.8533996 ,\n",
       "        2.672961  ,  2.4002647 , -1.5718228 , -1.1062899 ,  0.4144546 ,\n",
       "        1.1343325 ,  0.65221834,  0.7099155 ,  1.0796703 ,  2.918477  ,\n",
       "        0.08105207, -0.09059106, -2.914324  ,  0.12340591, -0.59207165,\n",
       "       -1.3185276 , -0.06577865, -1.9052536 , -0.8120679 ,  0.31761113,\n",
       "        0.23585844,  0.40274814,  3.0000565 ,  1.5815481 ,  0.01259109,\n",
       "        0.46309674, -0.23900114,  1.3251323 ,  0.693579  ,  1.341133  ,\n",
       "       -0.79970104,  0.0258701 ,  1.9583639 , -1.393357  , -0.32392883,\n",
       "        4.2385507 , -0.5818034 ,  1.2247164 ,  2.0337453 ,  4.663934  ,\n",
       "       -0.4016172 , -0.9713029 , -0.25434756, -2.7352052 , -1.726372  ,\n",
       "       -0.79812753,  2.324461  , -1.0246229 ,  1.2147952 , -0.42412892,\n",
       "        0.36500287,  2.026698  ,  0.5946051 , -1.9283456 , -1.4353302 ,\n",
       "       -0.3924454 ,  2.6522348 , -1.7252268 , -1.949522  ,  3.724274  ,\n",
       "        2.4289489 ,  1.281449  , -0.7739064 , -0.19489937, -1.8991859 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9bac8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the dataset is an imbalanced dataset, we shall assign higher weights for the bot examples\n",
    "#the weights assigned to the classes shall be a reciprocal of the frequencies\n",
    "class_weights = {0:1.334,1:3.992} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "07758092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating the neural network, starting with a relatively simple architechture\n",
    "model = Sequential()\n",
    "#create the embedding layer for the conversion of inputs to embedded vectors\n",
    "model.add(Embedding(vocabulary_size, output_dim = dim, weights = [embedding_mat], input_length = max_length, trainable = False))\n",
    "#All these LSTMS run for a 100 \"timesteps\" as determined by the max length parameter the units is the determine the outputsize\n",
    "model.add(LSTM(units = 256, recurrent_dropout=0.3))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1ef6f9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "940/940 [==============================] - 787s 830ms/step - loss: 1.2387 - acc: 0.7127 - val_loss: 0.5845 - val_acc: 0.7520\n",
      "Epoch 2/8\n",
      "940/940 [==============================] - 605s 643ms/step - loss: 1.2012 - acc: 0.7260 - val_loss: 0.5547 - val_acc: 0.7559\n",
      "Epoch 3/8\n",
      "940/940 [==============================] - 1232s 1s/step - loss: 1.1829 - acc: 0.7310 - val_loss: 0.5938 - val_acc: 0.7035\n",
      "Epoch 4/8\n",
      "661/940 [====================>.........] - ETA: 2:45 - loss: 1.1728 - acc: 0.7238"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [125]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train_tokens, y_train, validation_split = 0.3, epochs = 8, class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d000a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the preprocessing steps on the test set and seeing the performance\n",
    "X_test_sentence = [d.split() for d in X_test.text.tolist()]\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokens = pad_sequences(X_test_tokens, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0374194",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(model.predict(X_test_tokens) > 0.3, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84920f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape((y_pred.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df30a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = np.where(y_pred_train > 0.5, 1,0)\n",
    "y_pred_train = y_pred_train.reshape((y_pred_train.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d30780",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571f5143",
   "metadata": {},
   "source": [
    "## Import Packages and Reading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3a18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb # recommended version: 1.5.0\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score, RepeatedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import plot_importance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d139119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading of Excel\n",
    "dataset = pd.read_csv('cleaned_others_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d544eda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protected</th>\n",
       "      <th>verified</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>is_quoted_retweets</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>un_no_of_char</th>\n",
       "      <th>un_special_char</th>\n",
       "      <th>un_uppercase</th>\n",
       "      <th>name_no_of_char</th>\n",
       "      <th>name_special_char</th>\n",
       "      <th>name_uppercase</th>\n",
       "      <th>des_no_of_usertags</th>\n",
       "      <th>des_no_of_hashtags</th>\n",
       "      <th>des_external_links</th>\n",
       "      <th>has_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1997</td>\n",
       "      <td>5</td>\n",
       "      <td>17090</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>785</td>\n",
       "      <td>829</td>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>1232</td>\n",
       "      <td>1469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>749</td>\n",
       "      <td>838</td>\n",
       "      <td>2518</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>737048</td>\n",
       "      <td>128</td>\n",
       "      <td>4739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33581</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>367523</td>\n",
       "      <td>17291</td>\n",
       "      <td>24084</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33582</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33583</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>395</td>\n",
       "      <td>492</td>\n",
       "      <td>21437</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33584</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>618</td>\n",
       "      <td>3021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33585</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>803247</td>\n",
       "      <td>7</td>\n",
       "      <td>3159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33586 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       protected  verified  location  followers_count  following_count  \\\n",
       "0          False         0         0             1997                5   \n",
       "1          False         0         1              785              829   \n",
       "2          False         0         0              243             1232   \n",
       "3          False         0         1              749              838   \n",
       "4          False         1         1           737048              128   \n",
       "...          ...       ...       ...              ...              ...   \n",
       "33581      False         1         1           367523            17291   \n",
       "33582      False         0         1               14                0   \n",
       "33583      False         0         0              395              492   \n",
       "33584      False         0         0              159              618   \n",
       "33585      False         1         1           803247                7   \n",
       "\n",
       "       tweet_count  is_quoted_retweets  isFraud  un_no_of_char  \\\n",
       "0            17090                   0        1             15   \n",
       "1              251                   0        0             12   \n",
       "2             1469                   0        0              8   \n",
       "3             2518                   1        0             11   \n",
       "4             4739                   0        0             13   \n",
       "...            ...                 ...      ...            ...   \n",
       "33581        24084                   1        0             14   \n",
       "33582          238                   1        0              8   \n",
       "33583        21437                   0        1             14   \n",
       "33584         3021                   0        0              8   \n",
       "33585         3159                   0        0              9   \n",
       "\n",
       "       un_special_char  un_uppercase  name_no_of_char  name_special_char  \\\n",
       "0                    1             0               19                  0   \n",
       "1                    0             1                8                  0   \n",
       "2                    0             1               11                  0   \n",
       "3                    0             1               15                  0   \n",
       "4                    0             1               14                  0   \n",
       "...                ...           ...              ...                ...   \n",
       "33581                0             1               10                  0   \n",
       "33582                1             1                8                  0   \n",
       "33583                0             1               16                  0   \n",
       "33584                0             1               14                  0   \n",
       "33585                1             1               10                  1   \n",
       "\n",
       "       name_uppercase  des_no_of_usertags  des_no_of_hashtags  \\\n",
       "0                   1                   2                   0   \n",
       "1                   1                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   1                   0                   0   \n",
       "4                   1                   1                   0   \n",
       "...               ...                 ...                 ...   \n",
       "33581               1                   2                   0   \n",
       "33582               1                   0                   0   \n",
       "33583               1                   0                   0   \n",
       "33584               1                   0                   0   \n",
       "33585               0                   0                   0   \n",
       "\n",
       "       des_external_links  has_description  \n",
       "0                       0                1  \n",
       "1                       0                1  \n",
       "2                       0                1  \n",
       "3                       0                1  \n",
       "4                       0                1  \n",
       "...                   ...              ...  \n",
       "33581                   0                1  \n",
       "33582                   0                1  \n",
       "33583                   0                1  \n",
       "33584                   0                0  \n",
       "33585                   0                0  \n",
       "\n",
       "[33586 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b40aae",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "* Scale numerical features\n",
    "* One Hot Encode for categorical/ordinal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70acbf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (26868, 17) (26868,)\n",
      "Test set: (6718, 17) (6718,)\n"
     ]
    }
   ],
   "source": [
    "# Getting X and y\n",
    "x = dataset.drop(columns = ['isFraud'])\n",
    "y = dataset['isFraud']\n",
    "\n",
    "# Split into training and testing sets before scaling the variables and performing one hot encoding to avoid data leakage\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=101)\n",
    "print(\"Train set:\", x_train.shape, y_train.shape)\n",
    "print(\"Test set:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381970a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We only want to scale the numeric variables and not the categorical features. \n",
    "# # Hence, we create a columntransformer to help us to do this\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# x_train[['followers_count', 'following_count', 'tweet_count', 'un_no_of_char', 'name_no_of_char', 'des_no_of_usertags', 'des_no_of_hashtags']] = scaler.fit_transform(x_train[['followers_count', 'following_count', 'tweet_count', 'un_no_of_char','name_no_of_char', 'des_no_of_usertags', 'des_no_of_hashtags']])\n",
    "\n",
    "# x_test[['followers_count', 'following_count', 'tweet_count', 'un_no_of_char', 'name_no_of_char', 'des_no_of_usertags', 'des_no_of_hashtags']] = scaler.transform(x_test[['followers_count', 'following_count', 'tweet_count', 'un_no_of_char','name_no_of_char', 'des_no_of_usertags', 'des_no_of_hashtags']])\n",
    "\n",
    "# # Transform the year column into a categorical variable and store the result in a dataframe\n",
    "# encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "# transformed_year_train_matrix = encoder.fit_transform(x_train[['year']])\n",
    "# transformed_year_train = pd.DataFrame(transformed_year_train_matrix, columns = encoder.get_feature_names())\n",
    "\n",
    "# # Using the fitted encoder to transform the test data and storing in a dataframe\n",
    "# transformed_year_test_matrix = encoder.transform(x_test[['year']])\n",
    "# transformed_year_test = pd.DataFrame(transformed_year_test_matrix, columns = encoder.get_feature_names())\n",
    "\n",
    "# # Reset the index for training and testing sets to allow merging with the one hot encoded variables\n",
    "# # Drop the index column \n",
    "# x_train.reset_index(inplace = True, drop = True)\n",
    "# x_test.reset_index(inplace = True, drop = True)\n",
    "# y_train.reset_index(drop = True, inplace = True)\n",
    "# y_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# # Combine the scaled numeric variables together with the year categorical variables\n",
    "# x_train = pd.concat([x_train, transformed_year_train], axis = 1)\n",
    "# x_test = pd.concat([x_test, transformed_year_test], axis = 1)\n",
    "\n",
    "# # Drop the year column since we already have the dummy variables for year\n",
    "# x_train.drop(labels = ['year'], axis = 1, inplace = True)\n",
    "# x_test.drop(labels = ['year'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f6a8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that combines all the preprocessing steps (for model pipeline purpose)\n",
    "class ExperimentalTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, numeric_variables = ['followers_count', 'following_count', 'tweet_count', 'un_no_of_char','name_no_of_char', 'des_no_of_usertags', 'des_no_of_hashtags']):\n",
    "        self.encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "        self.scaler = StandardScaler()\n",
    "        #specified numeric variables, by default it is the above\n",
    "        self.numeric_variables = numeric_variables\n",
    "        self.columns = []\n",
    "    \n",
    "    #The fit function that will be called when this custom transformer is fit\n",
    "    def fit(self, X, y = None):\n",
    "        #fit the one hot encoder to the year\n",
    "#         self.encoder.fit(X[['year']])\n",
    "        #fit the scaler on the numeric variables\n",
    "        self.scaler.fit(X[self.numeric_variables])\n",
    "        return self\n",
    "    \n",
    "    #The transform function that will be called\n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        #to avoid changing the original dataset\n",
    "        X_ = X.copy()\n",
    "        #transforming the numeric variables according to the fitted scaler\n",
    "        X_[self.numeric_variables] = self.scaler.transform(X_[self.numeric_variables])\n",
    "        \n",
    "#         #next, create a date matrix using the fitted encoder\n",
    "#         transformed_year_matrix = self.encoder.transform(X_[['year']])\n",
    "#         #get the dataframe\n",
    "#         year_cols = list(map(lambda year: year.replace('x0', 'year'), self.encoder.get_feature_names()))\n",
    "#         transformed_year_train = pd.DataFrame(transformed_year_matrix, columns = year_cols)\n",
    "#         #reset the index of the original dataframe\n",
    "#         X_.reset_index(drop = True, inplace = True)\n",
    "#         X_ = pd.concat([X_, transformed_year_train], axis = 1)\n",
    "#         #drop the year column from the dataframe\n",
    "#         X_.drop(labels = ['year'], inplace = True, axis = 1)\n",
    "        self.columns = X_.columns\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94b9fe",
   "metadata": {},
   "source": [
    "## Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74dafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_chart(shap_values, x_test_values, names, model_type):\n",
    "# #     feature_importance_df = pd.DataFrame(model.coef_.T, x_train.columns.T,columns=['Feature Importance']).sort_values(by='Feature Importance', ascending=False)\n",
    "# #     fig = feature_importance_df.plot(kind=\"barh\", figsize=(20, 50))\n",
    "# #     bg = fig.patch\n",
    "# #     plt.gca().invert_yaxis()\n",
    "#     shap.summary_plot(shap_values, x_test_values, feature_names = names, plot_type = \"bar\", sort = True)\n",
    "#     plt.grid()\n",
    "# #     plt.savefig(f'Charts\\\\{model_type} Feature Importance.png', dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "# model = SVC()\n",
    "# model.fit(x_train, y_train)\n",
    "# explainer = shap.Explainer(model.predict, x_test, feature_perturbation=\"interventional\")\n",
    "# shap_values = explainer(x_test)\n",
    "# x_test_arr = x_test.toarray()\n",
    "# feature_chart(shap_values, x_test_arr, x_test.columns, model_type)\n",
    "    \n",
    "def feature_chart(feature_importance_df, model_type):\n",
    "#     feature_importance_df = pd.DataFrame(model.coef_.T, x_train.columns.T,columns=['Feature Importance']).sort_values(by='Feature Importance', ascending=False)\n",
    "    fig = feature_importance_df.plot(kind=\"barh\", figsize=(12, 15))\n",
    "    bg = fig.patch\n",
    "    bg.set_facecolor(\"white\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid()\n",
    "#     plt.savefig(f'Charts\\\\{model_type} Feature Importance.png', dpi=300, facecolor=fig.get_facecolor())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e922e",
   "metadata": {},
   "source": [
    "## 01 Logistic Regression\n",
    "* Model \n",
    "* Error Metrics\n",
    "* Plot for Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a55ea5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "log_regression = LogisticRegression()\n",
    "transformer = ExperimentalTransformer()\n",
    "log_model_pipeline = make_pipeline(transformer, log_regression)\n",
    "log_model = log_model_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b30e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "# filename = '01 Logistic Regression.sav'\n",
    "# pickle.dump(log_model, open(filename, 'wb'))\n",
    "\n",
    "# Load the model from disk\n",
    "# results_random = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fffa28f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7448265594759566"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R-Squared\n",
    "log_model.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bc6e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prediction\n",
    "y_pred = log_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaf1c7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 8.42138677482952\n",
      "ROC AUC: 0.6839353549998228\n",
      "F1-score: 0.5608579088471849\n",
      "Precision: 0.6987307949231797\n",
      "Recall: 0.46842812360053737\n"
     ]
    }
   ],
   "source": [
    "# Error Metrics\n",
    "lg_loss = metrics.log_loss(y_test, y_pred)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23963f",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcde3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature = pd.DataFrame(log_model.named_steps[\"logisticregression\"].coef_.T, log_model.named_steps[\"experimentaltransformer\"].columns, columns=['Feature Importance']).sort_values(by='Feature Importance', ascending=False)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9913e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_chart(feature, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1fe58",
   "metadata": {},
   "source": [
    "## 02 Random Forest\n",
    "* Model (Random Search + Grid Search)\n",
    "* Error Metrics\n",
    "* Plot for Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc165c",
   "metadata": {},
   "source": [
    "### Model (Random Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96628df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "rf_classifier = RandomForestClassifier()\n",
    "transformer = ExperimentalTransformer()\n",
    "rf_model = make_pipeline(transformer, rf_classifier)\n",
    "\n",
    "space = dict()\n",
    "\n",
    "# Number of trees in random forest\n",
    "space[\"randomforestclassifier__n_estimators\"] = [10, 50, 200, 600, 800, 1200]\n",
    "\n",
    "# Number of features to consider at every split (this parameter avoids overfitting by limiting how many features each leaf node can look at) \n",
    "space[\"randomforestclassifier__max_features\"] = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "# Maximum number of levels in tree (how deep the tree goes)\n",
    "space[\"randomforestclassifier__max_depth\"] = [None, 10, 40, 80, 120]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "space[\"randomforestclassifier__min_samples_split\"] = [2, 10, 40, 100]\n",
    "\n",
    "# Method of selecting samples for training each tree (bootstrap sampling or not)\n",
    "space[\"randomforestclassifier__bootstrap\"] = [True, False]\n",
    "\n",
    "# Define search\n",
    "random_search = RandomizedSearchCV(rf_model, \n",
    "                                   space, \n",
    "                                   scoring = 'f1',\n",
    "                                   cv = 5, \n",
    "                                   verbose=2, \n",
    "                                   random_state=123, \n",
    "                                   n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Execute search\n",
    "results_random_rf = random_search.fit(x_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "# filename = '02 Random Forest (random).sav'\n",
    "# pickle.dump(results_random, open(filename, 'wb'))\n",
    "\n",
    "# Load the model from disk\n",
    "# results_random = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "print('Best Score: %s' % results_random_rf.best_score_)\n",
    "print('Best Hyperparameters: %s' % results_random_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prediction for the best model using RandomSearchCV\n",
    "y_pred_random = results_random_rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe29cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Metrics\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_random)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_random)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_random)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_random)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_random)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c333b817",
   "metadata": {},
   "source": [
    "### Model (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "rf_classifier = RandomForestClassifier()\n",
    "transformer = ExperimentalTransformer()\n",
    "rf_model = make_pipeline(transformer, rf_classifier)\n",
    "\n",
    "grid = dict()\n",
    "\n",
    "# Number of trees in random forest\n",
    "grid[\"randomforestclassifier__n_estimators\"] = [10, 50, 200, 600, 800, 1200]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "grid[\"randomforestclassifier__max_features\"] = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "# Maximum number of levels in tree (how deep the tree goes)\n",
    "grid[\"randomforestclassifier__max_depth\"] = [None, 10, 40, 80, 120]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "grid[\"randomforestclassifier__min_samples_split\"] = [2, 10, 40, 100]\n",
    "\n",
    "# Method of selecting samples for training each tree (bootstrap sampling or not)\n",
    "grid[\"randomforestclassifier__bootstrap\"] = [True, False]\n",
    "\n",
    "# Define search\n",
    "grid_search = GridSearchCV(rf_model, \n",
    "                           grid, \n",
    "                           scoring = 'f1',\n",
    "                           cv = 5, \n",
    "                           n_jobs = -1, \n",
    "                           verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed1d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Execute search\n",
    "results_grid_rf = grid_search.fit(x_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d9f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "# filename = '02 Random Forest (grid).sav'\n",
    "# pickle.dump(results_grid, open(filename, 'wb'))\n",
    "\n",
    "# Load the model from disk\n",
    "# results_grid = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "print('Best Score: %s' % results_grid_rf.best_score_)\n",
    "print('Best Hyperparameters: %s' % results_grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prediction for the best model using GridSearchCV\n",
    "y_pred_grid = results_grid_rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4116fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Metrics\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_grid)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_grid)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_grid)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_grid)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_grid)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1fa204",
   "metadata": {},
   "source": [
    "### Model (Optimal Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa153269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid Search:\", grid_search_rf.best_score_) \n",
    "print(\"Randomised Search:\", random_search_rf.best_score_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (initialise the object based on parameters selected by grid search)\n",
    "rf_classifier = RandomForestClassifier(bootstrap=True, max_depth=80, max_features='auto', min_samples_split=2, n_estimators=800)\n",
    "transformer = ExperimentalTransformer()\n",
    "rf_model_pipeline = make_pipeline(transformer, rf_classifier)\n",
    "rf_model = rf_model_pipeline.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "# Save the model to disk\n",
    "# filename = '02 Random Forest (Optimal).sav'\n",
    "# pickle.dump(rf_model, open(filename, 'wb'))\n",
    "\n",
    "# Load the model from disk\n",
    "# rf_regression = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bda1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prediction for the best model\n",
    "y_pred_optimal = rf_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3129661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Metrics\n",
    "mae = metrics.accuracy_score(y_test, y_pred_optimal)\n",
    "print(f'Accuracy: {mae}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_optimal)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_optimal)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_optimal)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9affb9b",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81edf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_model.feature_importances_\n",
    "feature = pd.DataFrame(rf_model.named_steps[\"randomforestclaasifier\"].feature_importances_, rf_model.named_steps[\"experimentaltransformer\"].columns.columns.T, columns=['Feature Importance']).sort_values(by='Feature Importance', ascending=False)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a761d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_chart(feature, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32988753",
   "metadata": {},
   "source": [
    "## 03 XGBoost\n",
    "* Model (Random Search + Grid Search + Bayesian Optimisation)\n",
    "* Error Metrics\n",
    "* Plot for Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410e8c7",
   "metadata": {},
   "source": [
    "### Model (Random Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e516bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = xgb.XGBClassifier(gamma = 0.1,\n",
    "                                   alpha = 0.5,\n",
    "                                   objective = \"binary:logistic\",\n",
    "                                   eval_metric = \"logloss\")\n",
    "transformer = ExperimentalTransformer()\n",
    "xgb_model = make_pipeline(transformer, xgb_classifier)\n",
    "\n",
    "space = dict()\n",
    "\n",
    "# Maximum depth of the individual regression estimators\n",
    "space[\"xgbclassifier__max_depth\"] = [6, 10, 15, 20, 25]\n",
    "\n",
    "# Fraction of samples to be used for fitting the individual base learners\n",
    "space[\"xgbclassifier__subsample\"] = [0.6, 0.8, 1.0]\n",
    "\n",
    "# Step size shrinkage used in update to prevents overfitting\n",
    "space[\"xgbclassifier__eta\"] = [0.01, 0.1, 0.2]\n",
    "\n",
    "# Subsample ratio of columns when constructing each tree\n",
    "space[\"xgbclassifier__colsample_bytree\"] = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Define Search\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model,\n",
    "                                   param_distributions=parameters,\n",
    "                                   scoring = 'f1',\n",
    "                                   n_jobs = -1,\n",
    "                                   cv = 5,\n",
    "                                   verbose=10,\n",
    "                                   n_iter=50,\n",
    "                                   random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86645987",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Execute Search\n",
    "results_random_xgb = random_search.fit(x_train, y_train)\n",
    "# Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "# CPU times: user 1min 23s, sys: 3.7 s, total: 1min 27s\n",
    "# Wall time: 1h 29min 45s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'random_all.sav'\n",
    "# pickle.dump(random_search, open(filename, 'wb'))\n",
    "\n",
    "print('Best Score: %s' % results_random_xgb.best_score_)\n",
    "print('Best Hyperparameters: %s' % results_random_xgb.best_params_)\n",
    "\n",
    "# print(results_random.best_params_)\n",
    "# print(\"Randomised Search:\", results_random.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prediction for the best model using RandomSearchCV\n",
    "y_pred_random = results_random_xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2606cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Metrics\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_random)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_random)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_random)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_random)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_random)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e30755",
   "metadata": {},
   "source": [
    "### Model (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b21f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = xgb.XGBClassifier(gamma = 0.1,\n",
    "                                   alpha = 0.5,\n",
    "                                   objective = \"binary:logistic\",\n",
    "                                   eval_metric = \"logloss\")\n",
    "transformer = ExperimentalTransformer()\n",
    "xgb_model = make_pipeline(transformer, xgb_classifier)\n",
    "\n",
    "grid = dict()\n",
    "\n",
    "# Maximum depth of the individual regression estimators\n",
    "grid[\"xgbclassifier__max_depth\"] = [6, 10, 15, 20, 25]\n",
    "\n",
    "# Fraction of samples to be used for fitting the individual base learners\n",
    "grid[\"xgbclassifier__subsample\"] = [0.6, 0.8, 1.0]\n",
    "\n",
    "# Step size shrinkage used in update to prevents overfitting\n",
    "grid[\"xgbclassifier__eta\"] = [0.01, 0.1, 0.2]\n",
    "\n",
    "# Subsample ratio of columns when constructing each tree\n",
    "grid[\"xgbclassifier__colsample_bytree\"] = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model,\n",
    "                           param_grid=parameters,\n",
    "                           scoring = 'f1',\n",
    "                           n_jobs = -1,\n",
    "                           cv = 5,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results_grid_xgb = grid_search.fit(x_train, y_train)\n",
    "# Fitting 5 folds for each of 225 candidates, totalling 1125 fits\n",
    "# CPU times: user 2min 51s, sys: 15.3 s, total: 3min 6s\n",
    "# Wall time: 6h 58min 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67674160",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Score: %s' % results_grid_xgb.best_score_)\n",
    "print('Best Hyperparameters: %s' % results_grid_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prediction for the best model using GridSearchCV\n",
    "y_pred_grid = results_grid_xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21bdeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Metrics\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_grid)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_grid)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_grid)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_grid)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_grid)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f95af",
   "metadata": {},
   "source": [
    "### Model (Bayesian Optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8551a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_hyper_pram(eta, max_depth, subsample, gamma, colsample_bytree, alpha):\n",
    "    max_depth = int(max_depth)\n",
    "    xgb_classifier = xgb.XGBClassifier(max_depth = max_depth, \n",
    "                                       eta = eta, \n",
    "                                       gamma = gamma,\n",
    "                                       subsample = subsample,\n",
    "                                       colsample_bytree = colsample_bytree,\n",
    "                                       alpha = alpha,\n",
    "                                       objective = \"binary:logistic\",\n",
    "                                       eval_metric = \"logloss\")\n",
    "\n",
    "    transformer = ExperimentalTransformer()\n",
    "    xgb_model = make_pipeline(transformer, xgb_classifier)\n",
    "    \n",
    "    xgb_model.fit(x_train, y_train)\n",
    "    y_pred = xgb_model.predict(x_test)\n",
    "    \n",
    "    return (metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a24c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_model = BayesianOptimization(xgboost_hyper_pram, \n",
    "                                   {'max_depth' : (1, 30), \n",
    "                                   'eta' : (0, 0.2), \n",
    "                                   'gamma' : (0, 1),\n",
    "                                   'subsample' : (0, 1),\n",
    "                                   'colsample_bytree' : (0, 1),\n",
    "                                   'alpha' : (0, 1)},\n",
    "                                   random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8c2ce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   | colsam... |    eta    |   gamma   | max_depth | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| 1         | 0.7236    | 0.6965    | 0.2861    | 0.04537   | 0.5513    | 21.86     | 0.4231    |\n",
      "| 2         | 0.7613    | 0.9808    | 0.6848    | 0.09619   | 0.3921    | 10.95     | 0.729     |\n",
      "| 3         | 0.6429    | 0.4386    | 0.05968   | 0.07961   | 0.738     | 6.292     | 0.1755    |\n",
      "| 4         | 0.7527    | 0.5316    | 0.5318    | 0.1269    | 0.8494    | 22.01     | 0.611     |\n",
      "| 5         | 0.743     | 0.7224    | 0.323     | 0.07236   | 0.2283    | 9.518     | 0.631     |\n",
      "| 6         | 0.7537    | 0.0921    | 0.4337    | 0.08617   | 0.4937    | 13.35     | 0.3123    |\n",
      "| 7         | 0.7366    | 0.4264    | 0.8934    | 0.1888    | 0.5018    | 19.09     | 0.1156    |\n",
      "| 8         | 0.7488    | 0.3173    | 0.4148    | 0.1733    | 0.2505    | 15.01     | 0.9856    |\n",
      "| 9         | 0.755     | 0.5195    | 0.6129    | 0.02413   | 0.8263    | 18.49     | 0.5451    |\n",
      "| 10        | 0.7404    | 0.3428    | 0.3041    | 0.0834    | 0.6813    | 26.39     | 0.5104    |\n",
      "| 11        | 0.7498    | 0.6693    | 0.5859    | 0.125     | 0.6747    | 25.43     | 0.08319   |\n",
      "| 12        | 0.7003    | 0.7637    | 0.2437    | 0.03884   | 0.5725    | 3.776     | 0.8853    |\n",
      "| 13        | 0.7401    | 0.6272    | 0.7234    | 0.003226  | 0.5944    | 17.15     | 0.159     |\n",
      "| 14        | 0.7631    | 0.1531    | 0.6955    | 0.06375   | 0.692     | 17.08     | 0.389     |\n",
      "| 15        | 0.7633    | 0.9251    | 0.8417    | 0.07148   | 0.04359   | 9.838     | 0.3982    |\n",
      "| 16        | 0.7588    | 0.705     | 0.9954    | 0.07118   | 0.7625    | 18.2      | 0.6917    |\n",
      "| 17        | 0.7427    | 0.1511    | 0.3989    | 0.04817   | 0.3435    | 15.88     | 0.6666    |\n",
      "| 18        | 0.6789    | 0.1059    | 0.1309    | 0.0644    | 0.6616    | 25.55     | 0.5533    |\n",
      "| 19        | 0.7458    | 0.8545    | 0.3848    | 0.06336   | 0.3543    | 5.961     | 0.8291    |\n",
      "| 20        | 0.6879    | 0.3387    | 0.5524    | 0.1157    | 0.5215    | 1.078     | 0.9883    |\n",
      "| 21        | 0.7139    | 0.9053    | 0.2076    | 0.0585    | 0.52      | 27.16     | 0.9836    |\n",
      "| 22        | 0.7416    | 0.2575    | 0.5644    | 0.1614    | 0.3944    | 22.2      | 0.1611    |\n",
      "| 23        | 0.7411    | 0.6007    | 0.8659    | 0.1967    | 0.07937   | 13.42     | 0.2045    |\n",
      "| 24        | 0.7488    | 0.4506    | 0.5478    | 0.01867   | 0.2969    | 27.9      | 0.569     |\n",
      "| 25        | 0.7506    | 0.4574    | 0.7535    | 0.1484    | 0.04858   | 21.55     | 0.8392    |\n",
      "| 26        | 0.7532    | 0.7217    | 0.8784    | 0.0006844 | 0.9309    | 25.21     | 0.9074    |\n",
      "| 27        | 0.7563    | 0.8798    | 0.7416    | 0.02244   | 0.129     | 9.81      | 0.5675    |\n",
      "| 28        | 0.7571    | 0.6637    | 0.6959    | 0.09072   | 0.9707    | 17.82     | 0.08281   |\n",
      "| 29        | 0.7553    | 0.1936    | 0.7324    | 0.09759   | 0.8377    | 17.58     | 0.4994    |\n",
      "| 30        | 0.7615    | 0.9889    | 0.855     | 0.1267    | 0.1262    | 10.36     | 0.398     |\n",
      "| 31        | 0.0       | 1.0       | 1.0       | 0.0       | 0.261     | 9.984     | 0.07299   |\n",
      "| 32        | 0.752     | 0.03372   | 0.5838    | 0.117     | 0.6483    | 17.08     | 0.3726    |\n",
      "| 33        | 0.7603    | 0.6153    | 0.8103    | 0.04882   | 0.7942    | 18.34     | 0.618     |\n",
      "| 34        | 0.758     | 0.9801    | 0.786     | 0.15      | 0.1331    | 10.53     | 0.5411    |\n",
      "| 35        | 0.7561    | 0.9071    | 0.7959    | 0.1888    | 0.0       | 9.775     | 0.5117    |\n",
      "| 36        | 0.7627    | 0.8422    | 0.8715    | 0.09463   | 0.02378   | 9.88      | 0.4623    |\n",
      "| 37        | 0.7568    | 0.05444   | 0.6181    | 0.06163   | 0.7723    | 17.07     | 0.3638    |\n",
      "| 38        | 0.7596    | 0.9719    | 0.7349    | 0.1416    | 0.0       | 10.22     | 0.5811    |\n",
      "| 39        | 0.7527    | 0.534     | 0.3337    | 0.1824    | 0.5866    | 4.258     | 0.2602    |\n",
      "| 40        | 0.7552    | 0.2891    | 0.6485    | 0.09835   | 0.8021    | 17.34     | 0.2433    |\n",
      "=================================================================================================\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "optim_model.maximize(n_iter=15, init_points=25, acq='ei')\n",
    "\n",
    "# CPU times: user 22min 42s, sys: 3.66 s, total: 22min 46s\n",
    "# Wall time: 22min 41s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7fda226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7632662951296341\n",
      "Best Hyperparameters: {'alpha': 0.9251324896139861, 'colsample_bytree': 0.8416699969127163, 'eta': 0.07147951333663526, 'gamma': 0.04359146379904055, 'max_depth': 9.838274128921826, 'subsample': 0.398185681917981}\n"
     ]
    }
   ],
   "source": [
    "print('Best Score: %s' % optim_model.max[\"target\"])\n",
    "print('Best Hyperparameters: %s' % optim_model.max['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fa2ae",
   "metadata": {},
   "source": [
    "### Model (Optimal Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6177496e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search_xbg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20120/691440386.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Grid Search:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_search_xbg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Randomised Search:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_search_xgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Bayesian Optimisation:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_search_xbg' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Grid Search:\", grid_search_xbg.best_score_) \n",
    "print(\"Randomised Search:\", random_search_xgb.best_score_) \n",
    "print(\"Bayesian Optimisation:\", optim_model.max[\"target\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88861a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned hyperparameters using \n",
    "# params = grid_search.best_params_\n",
    "params = optim_model.max['params']\n",
    "xgb_classifier = xgb.XGBClassifier(max_depth = int(params['max_depth']), \n",
    "                                   eta = params['eta'], \n",
    "                                   gamma = params['gamma'],\n",
    "                                   subsample = params['subsample'],\n",
    "                                   colsample_bytree = params['colsample_bytree'],\n",
    "                                   alpha = params['alpha'],\n",
    "                                   objective = \"binary:logistic\",\n",
    "                                   eval_metric = \"logloss\")\n",
    "\n",
    "transformer = ExperimentalTransformer()\n",
    "xgb_model_pipeline = make_pipeline(transformer, xgb_classifier)\n",
    "xgb_model = xgb_model_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b87d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prediction for the best model\n",
    "y_pred_optimal = xgb_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d713d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8545698124441798\n",
      "F1-score: 0.7632662951296341\n",
      "Precision: 0.8315733896515312\n",
      "Recall: 0.7053291536050157\n"
     ]
    }
   ],
   "source": [
    "# Error Metrics\n",
    "mae = metrics.accuracy_score(y_test, y_pred_optimal)\n",
    "print(f'Accuracy: {mae}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_optimal)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_optimal)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_optimal)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ccb49",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35825edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "xgb.plot_importance(xgb_model.named_steps[\"xgbclassifier\"], importance_type = 'gain', height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d0ac9",
   "metadata": {},
   "source": [
    "## 04 Support Vector Machine\n",
    "* Model (Random Search + Grid Search)\n",
    "* Error Metrics\n",
    "* Plot for Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641c483",
   "metadata": {},
   "source": [
    "### Model: Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ab427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "svm_classifier = SVC()\n",
    "transformer = ExperimentalTransformer()\n",
    "svm_model = make_pipeline(transformer, svm_classifier)\n",
    "\n",
    "space = dict()\n",
    "\n",
    "# Kernel type to be used in the algorithm\n",
    "space[\"svc__kernel\"] = ['poly', 'rbf', 'sigmoid']\n",
    "\n",
    "# Degree of the polynomial kernel function\n",
    "space[\"svc__degree\"] = [1, 3, 8]\n",
    "\n",
    "# Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "space[\"svc__gamma\"] = ['scale', 'auto']\n",
    "\n",
    "# Regularisation parameter\n",
    "space[\"svc__C\"] = [0.1, 1, 10, 100]\n",
    "\n",
    "# Enable verbose output\n",
    "space[\"svc__verbose\"] = [True, False]\n",
    "\n",
    "# Define search\n",
    "random_search = RandomizedSearchCV(svm_model, \n",
    "                                   space, \n",
    "                                   cv = 5, \n",
    "                                   scoring = 'f1',\n",
    "                                   verbose = 2, \n",
    "                                   random_state = 123, \n",
    "                                   n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6de847",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Execute search\n",
    "results_random_svm = random_search.fit(x_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa337ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "# filename = '04 SVM (Random).sav'\n",
    "# pickle.dump(results_random, open(filename, 'wb'))\n",
    "\n",
    "# Load the model from disk\n",
    "# results_random = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "print('Best Score: %s' % results_random_svm.best_score_)\n",
    "print('Best Hyperparameters: %s' % results_random_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prediction for the best model using RandomSearchCV\n",
    "y_pred_random = results_random_svm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d653d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Metrics\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_random)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_random)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_random)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_random)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_random)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41db33",
   "metadata": {},
   "source": [
    "### Model: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "svm_classifier = SVC()\n",
    "transformer = ExperimentalTransformer()\n",
    "svm_model = make_pipeline(transformer, svm_classifier)\n",
    "\n",
    "grid = dict()\n",
    "\n",
    "# Kernel type to be used in the algorithm\n",
    "grid[\"svc__kernel\"] = ['poly', 'rbf', 'sigmoid']\n",
    "\n",
    "# Degree of the polynomial kernel function\n",
    "grid[\"svc__degree\"] = [1, 3, 8]\n",
    "\n",
    "# Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "grid[\"svc__gamma\"] = ['scale', 'auto']\n",
    "\n",
    "# Regularisation parameter\n",
    "grid[\"svc__C\"] = [0.1, 1, 10, 100]\n",
    "\n",
    "# Enable verbose output\n",
    "grid[\"svc__verbose\"] = [True, False]\n",
    "\n",
    "# Define search\n",
    "search = GridSearchCV(svm_model, \n",
    "                      grid, \n",
    "                      scoring = 'f1',\n",
    "                      cv = 5, \n",
    "                      n_jobs = -1, \n",
    "                      verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d64eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Execute search\n",
    "results_grid_svm = search.fit(x_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda90380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "# filename = '04 SVM (Grid).sav'\n",
    "# pickle.dump(results_grid, open(filename, 'wb'))\n",
    "\n",
    "# Load the model from disk\n",
    "# results_grid = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "print('Best Score: %s' % results_grid_svm.best_score_)\n",
    "print('Best Hyperparameters: %s' % results_grid_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prediction for the best model using GridSearchCV\n",
    "y_pred_grid = results_grid_svm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfc19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Metrics\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_grid)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_grid)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_grid)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_grid)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_grid)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db31f44",
   "metadata": {},
   "source": [
    "### Model: Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid Search:\", grid_search_svm.best_score_) \n",
    "print(\"Randomised Search:\", random_search_svm.best_score_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Best Hyperparameters (Random): {'svc__verbose': False, 'svc__kernel': 'rbf', 'svc__gamma': 'auto', 'svc__degree': 1, 'svc__C': 100}\n",
    "# Best Hyperparameters (Grid): {'C': 10, 'degree': 8, 'gamma': 'scale', 'kernel': 'poly', 'verbose': True}\n",
    "# random_state = 123\n",
    "svm_classifier = SVC(C = 10, degree = 8, gamma = 'scale', kernel = 'poly', verbose = True, random_state = 123)\n",
    "transformer = ExperimentalTransformer()\n",
    "svm_model_pipeline = make_pipeline(transformer, svm_classifier)\n",
    "svm_model = svm_model_pipeline.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "# Save the model to disk\n",
    "# filename = '04 SVM (Optimal).sav'\n",
    "# pickle.dump(svm_model, open(filename, 'wb'))\n",
    "\n",
    "# Load the model from disk\n",
    "# rf_regression = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b950d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prediction for the best model\n",
    "y_pred_optimal = svm_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Metrics\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_optimal)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_optimal)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_optimal)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_optimal)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_optimal)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d7608",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "* Cannot find cause black box model unless use SHAP or perm importance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecfe153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "x_test_imps = svm_model.named_steps[\"experimentaltransformer\"].transform(x_test)\n",
    "imps = permutation_importance(svm_model.named_steps[\"svc\"], x_test_imps, y_test, random_state=123)\n",
    "feature = pd.DataFrame(imps.importances_mean, svm_model.named_steps[\"experimentaltransformer\"].columns.T, columns=['Feature Importance']).sort_values(by='Feature Importance', ascending=False)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_chart(feature, \"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dbab98",
   "metadata": {},
   "source": [
    "# NEED TO EDIT ONCE JES REUPLOADS\n",
    "## 05 Naive Bayes\n",
    "* Model (Random Search + Grid Search)\n",
    "* Error Metrics\n",
    "* Plot for Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040afd0",
   "metadata": {},
   "source": [
    "### Model (Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef683aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "nb_classifier = GaussianNB()\n",
    "transformer = ExperimentalTransformer()\n",
    "nb_model = make_pipeline(transformer, nb_classifier)\n",
    "\n",
    "space = dict()\n",
    "\n",
    "# \n",
    "space[\"gaussiannb__var_smoothing\"] = np.logspace(0,-9, num=200) \n",
    "    \n",
    "random_search = RandomizedSearchCV(estimator=nb_model,\n",
    "                                   param_distributions=space,\n",
    "                                   scoring = 'f1',\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=5,\n",
    "                                   verbose=10,\n",
    "                                   n_iter=50,\n",
    "                                   random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result_random_nb = random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cedfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Score: %s' % result_random_nb.best_score_)\n",
    "print('Best Hyperparameters: %s' % result_random_nb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_random = result_random_nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_loss = metrics.log_loss(y_test, y_pred_random)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_random)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "precision = metrics.precision_score(y_test, y_pred_random)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_random)\n",
    "print(f'Recall: {recall}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_random)\n",
    "print(f'F1-score: {f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34264c",
   "metadata": {},
   "source": [
    "### Model (Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "nb_classifier = GaussianNB()\n",
    "transformer = ExperimentalTransformer()\n",
    "nb_model = make_pipeline(transformer, nb_classifier)\n",
    "\n",
    "grid = dict()\n",
    "\n",
    "# \n",
    "grid[\"gaussiannb__var_smoothing\"] = np.logspace(0,-9, num=200) \n",
    "    \n",
    "grid_search = RandomizedSearchCV(estimator=nb_model,\n",
    "                                 param_distributions=grid,\n",
    "                                 scoring = 'f1',\n",
    "                                 n_jobs=-1,\n",
    "                                 cv=5,\n",
    "                                 verbose=10,\n",
    "                                 n_iter=50,\n",
    "                                 random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727cb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result_grid_nb = grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3476969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Score: %s' % result_grid_nb.best_score_)\n",
    "print('Best Hyperparameters: %s' % result_grid_nb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grid = result_grid_nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_loss = metrics.log_loss(y_test, y_pred_grid)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_grid)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "precision = metrics.precision_score(y_test, y_pred_grid)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_grid)\n",
    "print(f'Recall: {recall}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_grid)\n",
    "print(f'F1-score: {f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16489f3",
   "metadata": {},
   "source": [
    "### Model (Optimal Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f79a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid Search:\", grid_search_nb.best_score_) \n",
    "print(\"Randomised Search:\", random_search_nb.best_score_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f313ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# \n",
    "nb_classifier = GaussianNB(var_smoothing = 6.985879746785249e-06)\n",
    "transformer = ExperimentalTransformer()\n",
    "nb_model_pipeline = make_pipeline(transformer, nb_classifier)\n",
    "nb_model = nb_model_pipeline.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "# Save the model to disk\n",
    "# filename = '04 SVM (Optimal).sav'\n",
    "# pickle.dump(svm_model, open(filename, 'wb'))\n",
    "\n",
    "# Load the model from disk\n",
    "# rf_regression = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prediction for the best model\n",
    "y_pred_optimal = nb_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8dacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Metrics\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_optimal)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_optimal)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_optimal)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_optimal)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_optimal)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677deab9",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "x_test_imps = nb_model.named_steps[\"experimentaltransformer\"].transform(x_test)\n",
    "imps = permutation_importance(nb_model.named_steps[\"gaussiannb\"], x_test_imps, y_test, random_state=123)\n",
    "feature = pd.DataFrame(imps.importances_mean, nb_model.named_steps[\"experimentaltransformer\"].columns.T, columns=['Feature Importance']).sort_values(by='Feature Importance', ascending=False)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_chart(feature, \"GaussianNB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49bb99",
   "metadata": {},
   "source": [
    "## 06 Ensemble Model\n",
    "* Model (Random Search + Grid Search)\n",
    "* Error Metrics\n",
    "* Plot for Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d044cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert after getting optimal hyperparameters!!!!!\n",
    "rf_classifier = RandomForestClassifier()\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "svm_classifier = SVC()\n",
    "log_regression = LogisticRegression()\n",
    "\n",
    "transformer = ExperimentalTransformer()\n",
    "rf_model_pipeline = make_pipeline(transformer, rf_classifier)\n",
    "xgb_model_pipeline = make_pipeline(transformer, xgb_classifier)\n",
    "svm_model_pipeline = make_pipeline(transformer, svm_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba08fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = [rf_model_pipeline, xgb_model_pipeline, svm_model_pipeline]\n",
    "\n",
    "for algo in clf:\n",
    "    score = cross_val_score(algo, x_test, y_test, cv = 7, scoring = 'f1')\n",
    "    print(\"The f1 score of {} is:\".format(algo),score.mean())\n",
    "\n",
    "clf = [('rfc', rf_model_pipeline), ('xgb', xgb_model_pipeline), ('svm', svm_model_pipeline)] #list of (str, estimator)\n",
    "\n",
    "stack_model_pipeline = StackingClassifier(estimators = clf, final_estimator = log_regression)\n",
    "score = cross_val_score(stack_model_pipeline, x_test, y_test, cv = 7, scoring = 'f1')\n",
    "print(score)\n",
    "print(\"The f1 score of is:\", score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "stack_model = stack_model_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab87520",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2bb384",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stack = stack_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Metrics\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_stack)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_stack)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_stack)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_stack)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_stack)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774fd72d",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53617ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_imps = stack_model.transform(x_test)\n",
    "\n",
    "rf_class = stack_model.estimators_[0].named_steps[\"randomforestclassifier\"]\n",
    "xgb_class = stack_model.estimators_[1].named_steps[\"xgbclassifier\"]\n",
    "svm_class = stack_model.estimators_[2].named_steps[\"svc\"]\n",
    "\n",
    "rf_imps = permutation_importance(rf_class, x_test_imps, y_test, random_state=123)\n",
    "xgb_imps = permutation_importance(xgb_class, x_test_imps, y_test, random_state=123)\n",
    "svm_imps = permutation_importance(svm_class, x_test_imps, y_test, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_xgb_imps = np.add(rf_imps.importances_mean, xgb_imps.importances_mean)\n",
    "rf_xgb_svm_imps = np.add(rf_xgb_imps, svm_imps.importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efbc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature = pd.DataFrame(rf_xgb_svm_imps, x_test_imps.columns.T, columns=['Feature Importance']).sort_values(by='Feature Importance', ascending=False)\n",
    "feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

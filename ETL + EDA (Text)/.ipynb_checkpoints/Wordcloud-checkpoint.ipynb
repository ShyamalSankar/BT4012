{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64649161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing of necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from googletrans import Translator\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c4f89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\veena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\veena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\veena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\veena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09027c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the cleaned tweets dataset\n",
    "tweets = pd.read_csv(\"tweets_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab67d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For reference:\n",
    "#Preprocessing already performed:Extract emojis, extracted tagged accounts, removed URLs, changed to lower case, removed punctuation,\n",
    "# removed non-english tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe for the wordcloud\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "bot_tweets = tweets[tweets.isBot == 'bot'][['text']].copy()\n",
    "human_tweets = tweets[tweets.isBot == 'human'][['text']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d112cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6114b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "bot_tweets['tokenized'] = bot_tweets.text.apply(word_tokenize)\n",
    "contextual_stop = ['day','new','one', 'time','need']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f9b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_tweets['processed_data'] = bot_tweets.tokenized.apply(lambda x: [word.casefold() for word in x if word.casefold() not in stop\n",
    "                                                                    and word not in contextual_stop])\n",
    "bot_tweets['processed_data'] = bot_tweets.processed_data.apply(lambda x:[lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_tweets['sentence'] = bot_tweets['processed_data'].apply(lambda x:' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word cloud for bots\n",
    "big_sentence = ''\n",
    "\n",
    "for sentence in bot_tweets['sentence']:\n",
    "    big_sentence += sentence\n",
    "    \n",
    "wordcloud = WordCloud(background_color='white').generate(text = big_sentence)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77534a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly, let us create a wordcloud for humans\n",
    "human_tweets['tokenized'] = human_tweets.text.apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab28159",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_tweets['processed_data'] = human_tweets.tokenized.apply(lambda x: [word.casefold() for word in x if word.casefold() not in stop\n",
    "                                                                        and word not in contextual_stop])\n",
    "human_tweets['processed_data'] = human_tweets.processed_data.apply(lambda x:[lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_tweets['sentence'] = human_tweets['processed_data'].apply(lambda x:' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53742ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_sentence = ''\n",
    "\n",
    "for sentence in human_tweets['sentence']:\n",
    "    big_sentence += sentence\n",
    "    \n",
    "wordcloud = WordCloud(background_color = \"white\").generate(text = big_sentence)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

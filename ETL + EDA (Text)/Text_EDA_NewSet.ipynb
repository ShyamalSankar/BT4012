{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import language_tool_python\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import ssl\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Listen to This Charming Man by The Smiths  htt...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wish i can i would be seeing other hoes on the...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The decade in the significantly easier schedul...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Theim class=\\\"alignnone size-full wp-image-60...</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25567</th>\n",
       "      <td>2553</td>\n",
       "      <td>“The best kept secret</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25568</th>\n",
       "      <td>2554</td>\n",
       "      <td>Love the Choose your own adventure style of th...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25569</th>\n",
       "      <td>2555</td>\n",
       "      <td>JOIN OUR TEAM: Sneezing</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25570</th>\n",
       "      <td>2556</td>\n",
       "      <td>These deeply discounted 256GB SanDisk flash dr...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25571</th>\n",
       "      <td>2557</td>\n",
       "      <td>@deleonfc6 diamond pearl and platinum were hon...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text  \\\n",
       "0               0                             YEA now that note GOOD   \n",
       "1               1  Listen to This Charming Man by The Smiths  htt...   \n",
       "2               2  wish i can i would be seeing other hoes on the...   \n",
       "3               3  The decade in the significantly easier schedul...   \n",
       "4               4  \"Theim class=\\\"alignnone size-full wp-image-60...   \n",
       "...           ...                                                ...   \n",
       "25567        2553                              “The best kept secret   \n",
       "25568        2554  Love the Choose your own adventure style of th...   \n",
       "25569        2555                            JOIN OUR TEAM: Sneezing   \n",
       "25570        2556  These deeply discounted 256GB SanDisk flash dr...   \n",
       "25571        2557  @deleonfc6 diamond pearl and platinum were hon...   \n",
       "\n",
       "      account.type  \n",
       "0              bot  \n",
       "1            human  \n",
       "2              bot  \n",
       "3              bot  \n",
       "4              bot  \n",
       "...            ...  \n",
       "25567          bot  \n",
       "25568        human  \n",
       "25569          bot  \n",
       "25570        human  \n",
       "25571        human  \n",
       "\n",
       "[25572 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"tweets_data.csv\")\n",
    "tweets['account.type'] = 'human'\n",
    "tweets.loc[tweets['isBot'] == 1, 'account.type'] = 'bot'\n",
    "tweets.drop('isBot', axis = 1, inplace = True)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veena\\AppData\\Local\\Temp\\ipykernel_35024\\2451328056.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets[\"hashtags\"][word] = hashtag_list\n",
      "C:\\Users\\veena\\AppData\\Local\\Temp\\ipykernel_35024\\2451328056.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets[\"mentions\"][word] = mentions_list\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tweets['hashtags'] =''\n",
    "tweets['mentions'] =''\n",
    "for word in range(len(tweets)):\n",
    "    tweet = tweets[\"text\"][word]\n",
    "    tweet.replace('\\n', \" \")\n",
    "    word_list = tweet.split(' ')\n",
    "    regex = \"#(\\w+)\"\n",
    "    regex2 = \"@(\\w+)\"\n",
    "    hashtag_list = re.findall(regex, tweet)\n",
    "    mentions_list = re.findall(regex2, tweet)\n",
    "    tweets[\"hashtags\"][word] = hashtag_list\n",
    "    tweets[\"mentions\"][word] = mentions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "      <td>bot</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Listen to This Charming Man by The Smiths  htt...</td>\n",
       "      <td>human</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wish i can i would be seeing other hoes on the...</td>\n",
       "      <td>bot</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The decade in the significantly easier schedul...</td>\n",
       "      <td>bot</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Theim class=\\\"alignnone size-full wp-image-60...</td>\n",
       "      <td>bot</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25567</th>\n",
       "      <td>2553</td>\n",
       "      <td>“The best kept secret</td>\n",
       "      <td>bot</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25568</th>\n",
       "      <td>2554</td>\n",
       "      <td>Love the Choose your own adventure style of th...</td>\n",
       "      <td>human</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25569</th>\n",
       "      <td>2555</td>\n",
       "      <td>JOIN OUR TEAM: Sneezing</td>\n",
       "      <td>bot</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25570</th>\n",
       "      <td>2556</td>\n",
       "      <td>These deeply discounted 256GB SanDisk flash dr...</td>\n",
       "      <td>human</td>\n",
       "      <td>[]</td>\n",
       "      <td>[pcworld]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25571</th>\n",
       "      <td>2557</td>\n",
       "      <td>@deleonfc6 diamond pearl and platinum were hon...</td>\n",
       "      <td>human</td>\n",
       "      <td>[]</td>\n",
       "      <td>[deleonfc6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text  \\\n",
       "0               0                             YEA now that note GOOD   \n",
       "1               1  Listen to This Charming Man by The Smiths  htt...   \n",
       "2               2  wish i can i would be seeing other hoes on the...   \n",
       "3               3  The decade in the significantly easier schedul...   \n",
       "4               4  \"Theim class=\\\"alignnone size-full wp-image-60...   \n",
       "...           ...                                                ...   \n",
       "25567        2553                              “The best kept secret   \n",
       "25568        2554  Love the Choose your own adventure style of th...   \n",
       "25569        2555                            JOIN OUR TEAM: Sneezing   \n",
       "25570        2556  These deeply discounted 256GB SanDisk flash dr...   \n",
       "25571        2557  @deleonfc6 diamond pearl and platinum were hon...   \n",
       "\n",
       "      account.type hashtags     mentions  \n",
       "0              bot       []           []  \n",
       "1            human       []           []  \n",
       "2              bot       []           []  \n",
       "3              bot       []           []  \n",
       "4              bot       []           []  \n",
       "...            ...      ...          ...  \n",
       "25567          bot       []           []  \n",
       "25568        human       []           []  \n",
       "25569          bot       []           []  \n",
       "25570        human       []    [pcworld]  \n",
       "25571        human       []  [deleonfc6]  \n",
       "\n",
       "[25572 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No java install detected. Please install java to use language-tool-python.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tool \u001b[38;5;241m=\u001b[39m \u001b[43mlanguage_tool_python\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLanguageTool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men-US\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_of_grammar_errors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tweets)):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\server.py:62\u001b[0m, in \u001b[0;36mLanguageTool.__init__\u001b[1;34m(self, language, motherTongue, remote_server, newSpellings, new_spellings_persist, host, config)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_remote_server_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_is_alive():\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_server_on_free_port\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\server.py:238\u001b[0m, in \u001b[0;36mLanguageTool._start_server_on_free_port\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/v2/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_port)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_local_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServerError:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\server.py:248\u001b[0m, in \u001b[0;36mLanguageTool._start_local_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_start_local_server\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# Before starting local server, download language tool if needed.\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m     \u001b[43mdownload_lt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\download_lt.py:144\u001b[0m, in \u001b[0;36mdownload_lt\u001b[1;34m()\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(download_folder)\n\u001b[0;32m    138\u001b[0m old_path_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    139\u001b[0m     path \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLanguageTool*\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path)\n\u001b[0;32m    142\u001b[0m ]\n\u001b[1;32m--> 144\u001b[0m \u001b[43mconfirm_java_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m version \u001b[38;5;241m=\u001b[39m LATEST_VERSION\n\u001b[0;32m    146\u001b[0m filename \u001b[38;5;241m=\u001b[39m FILENAME\u001b[38;5;241m.\u001b[39mformat(version\u001b[38;5;241m=\u001b[39mversion)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\download_lt.py:75\u001b[0m, in \u001b[0;36mconfirm_java_compatibility\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m java_path \u001b[38;5;241m=\u001b[39m find_executable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m java_path:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Just ignore this and assume an old version of Java. It might not be\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# found because of a PATHEXT-related issue\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# (https://bugs.python.org/issue2200).\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo java install detected. Please install java to use language-tool-python.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     77\u001b[0m output \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mcheck_output([java_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-version\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     78\u001b[0m                                  stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT,\n\u001b[0;32m     79\u001b[0m                                  universal_newlines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     81\u001b[0m major_version, minor_version \u001b[38;5;241m=\u001b[39m parse_java_version(output)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No java install detected. Please install java to use language-tool-python."
     ]
    }
   ],
   "source": [
    "\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "tweets['no_of_grammar_errors'] = ''\n",
    "for word in range(len(tweets)):\n",
    "    cap = tweets[\"text\"][word]\n",
    "    word_list = cap.split(' ')\n",
    "    tweets['no_of_grammar_errors'][word] = len(tool.check(cap))\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['num_emojis'] = ''\n",
    "tweets['emojis'] = ''\n",
    "for word in range(len(tweets)):\n",
    "    tweet = tweets[\"text\"][word]\n",
    "    tweet.replace('\\n', \" \")\n",
    "    regex = \"<U+.*?>\"\n",
    "    emoji_list = re.findall(regex, tweet)\n",
    "    tweets[\"num_emojis\"][word] = len(emoji_list)\n",
    "    tweets[\"emojis\"][word] = emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets['mentions_count'] = tweets['mentions'].apply(lambda x: len(x))\n",
    "tweets['hashtags_count'] = tweets['hashtags'].apply(lambda x: len(x))\n",
    "human_tweets = tweets[tweets['account.type']== 'human']\n",
    "bot_tweets = tweets[tweets['account.type']== 'bot']\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grammar Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=tweets, x=\"account.type\", y=\"no_of_grammar_errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(data=tweets, x=\"account.type\", y=\"no_of_grammar_errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=tweets, x=\"account.type\", y=\"mentions_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(data=tweets, x=\"account.type\", y=\"mentions_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=tweets, x=\"account.type\", y=\"hashtags_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(data=tweets, x=\"account.type\", y=\"hashtags_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=tweets, x=\"account.type\", y=\"num_emojis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(data=tweets, x=\"account.type\", y=\"num_emojis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_tweets['no_of_grammar_errors'] = human_tweets['no_of_grammar_errors'].apply(int)\n",
    "human_tweets['num_emojis'] = human_tweets['num_emojis'].apply(int)\n",
    "relevant_values_human = human_tweets[['no_of_grammar_errors','hashtags_count','num_emojis','mentions_count']]\n",
    "sns.heatmap(relevant_values_human.corr(),cmap ='coolwarm', annot=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_tweets['no_of_grammar_errors'] = bot_tweets['no_of_grammar_errors'].apply(int)\n",
    "bot_tweets['num_emojis'] = bot_tweets['num_emojis'].apply(int)\n",
    "relevant_values_bot = bot_tweets[['no_of_grammar_errors','hashtags_count','num_emojis','mentions_count']]\n",
    "sns.heatmap(relevant_values_bot.corr(),cmap ='coolwarm', annot=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(relevant_values_human) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(relevant_values_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import textblob\n",
    "# from textblob import Word\n",
    "\n",
    "# tweets['incorrect_spell_count'] = ''\n",
    "# tweets['incorrect_spell_words'] = ''\n",
    "# for word in range(len(tweets)):\n",
    "#     cap = tweets[\"text\"][word]\n",
    "#     word_list = cap.split(' ')\n",
    "#     incorrect_words = []\n",
    "#     for i in word_list:\n",
    "#         if Word(i).spellcheck()[0][1] < 1.0:\n",
    "#             incorrect_words.append(i)\n",
    "#     # new_caption = \" \".join(correct_words)\n",
    "#     # captions[\"clean_captions\"][word] = new_caption\n",
    "#     tweets['incorrect_spell_words'][word] = incorrect_words\n",
    "#     tweets['incorrect_spell_count'][word] = len(incorrect_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(df):\n",
    "    for word in range(len(df)):\n",
    "        pattern = '[0-9]'\n",
    "        tweet_clean = df['text'][word].split(' ')\n",
    "        tweet_clean = [re.sub(pattern, '', i) for i in tweet_clean]\n",
    "        new_tweet = \" \".join(tweet_clean)\n",
    "        df['text'][word] = new_tweet\n",
    "    return df\n",
    "tweets = remove_numbers(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_tweets = tweets.loc[tweets['account.type'] == 'bot']\n",
    "human_tweets = tweets.loc[tweets['account.type'] == 'human']\n",
    "bot_tweets_text = bot_tweets[\"text\"]\n",
    "human_tweets_text = human_tweets[\"text\"]\n",
    "human_tweets_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('all')\n",
    "\n",
    "bot_tweets_tokens = []\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "for sentence in bot_tweets_text:\n",
    "    bot_tweets_tokens.append(word_tokenize(sentence))\n",
    "bot_tweets_tokens_2 = []\n",
    "for sentence in bot_tweets_tokens:\n",
    "    sentence2 = []\n",
    "    for word in sentence:\n",
    "        if word not in stopwords.words('english'):\n",
    "            sentence2.append(word)\n",
    "    bot_tweets_tokens_2.append(sentence2)\n",
    "bot_tweets_tokens_2\n",
    "bot_tweets_tokens_3 = []\n",
    "for sentence in bot_tweets_tokens_2:\n",
    "    sentence3 = []\n",
    "    for word in sentence:\n",
    "        sentence3.append(lemma.lemmatize(word))\n",
    "    bot_tweets_tokens_3.append(sentence3)\n",
    "len(bot_tweets_tokens_3)\n",
    "bot_tweets_tokens_4 = []\n",
    "for sentence in bot_tweets_tokens_3:\n",
    "    sentence4 = []\n",
    "    for word in sentence:\n",
    "        sentence4.append(PorterStemmer().stem(word))\n",
    "    bot_tweets_tokens_4.append(sentence4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_tweets_tokens = []\n",
    "for sentence in human_tweets_text:\n",
    "    human_tweets_tokens.append(word_tokenize(sentence))\n",
    "human_tweets_tokens_2 = []\n",
    "for sentence in human_tweets_tokens:\n",
    "    sentence2 = []\n",
    "    for word in sentence:\n",
    "        if word not in stopwords.words('english'):\n",
    "            sentence2.append(word)\n",
    "    human_tweets_tokens_2.append(sentence2)\n",
    "human_tweets_tokens_2\n",
    "human_tweets_tokens_3 = []\n",
    "for sentence in human_tweets_tokens_2:\n",
    "    sentence3 = []\n",
    "    for word in sentence:\n",
    "        sentence3.append(lemma.lemmatize(word))\n",
    "    human_tweets_tokens_3.append(sentence3)\n",
    "len(human_tweets_tokens_3)\n",
    "human_tweets_tokens_4 = []\n",
    "for sentence in human_tweets_tokens_3:\n",
    "    sentence4 = []\n",
    "    for word in sentence:\n",
    "        sentence4.append(PorterStemmer().stem(word))\n",
    "    human_tweets_tokens_4.append(sentence4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(bot_tweets_tokens_4)\n",
    "human_tweets_str = []\n",
    "for i in range(len(human_tweets_tokens_4)):\n",
    "    human_tweets_str.append(\" \".join(human_tweets_tokens_4[i]))\n",
    "vectorizer = CountVectorizer().fit(human_tweets_str)\n",
    "human_tweets_tfidf = TfidfTransformer().fit_transform(vectorizer.transform(human_tweets_str))\n",
    "\n",
    "bot_tweets_str = []\n",
    "for i in range(len(bot_tweets_tokens_4)):\n",
    "    bot_tweets_str.append(\" \".join(bot_tweets_tokens_4[i]))\n",
    "bot_tweets_tfidf = TfidfTransformer().fit_transform(vectorizer.transform(bot_tweets_str))\n",
    "(human_tweets_tfidf, bot_tweets_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Vocabulary size\n",
    "\n",
    "human_tweets_str = []\n",
    "for i in range(len(human_tweets_tokens_4)):\n",
    "    human_tweets_str.append(\" \".join(human_tweets_tokens_4[i]))\n",
    "human_tweets_countvec = CountVectorizer().fit_transform(human_tweets_str)\n",
    "\n",
    "bot_tweets_str = []\n",
    "for i in range(len(bot_tweets_tokens_4)):\n",
    "    bot_tweets_str.append(\" \".join(bot_tweets_tokens_4[i]))\n",
    "bot_tweets_countvec = CountVectorizer().fit_transform(bot_tweets_str)\n",
    "\n",
    "(human_tweets_countvec, bot_tweets_countvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_matrix = human_tweets_tfidf * csr_matrix.transpose(bot_tweets_tfidf)\n",
    "new_matrix\n",
    "\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "plt.rcParams[\"figure.figsize\"] = [7.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "im = ax1.imshow(new_matrix.todense(), cmap=\"copper_r\", vmin=0, vmax = 0.05)\n",
    "plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot Product between Bot and Bot Tweets\n",
    "dot_prod_matrix_bots = bot_tweets_tfidf * csr_matrix.transpose(bot_tweets_tfidf)\n",
    "# sns.heatmap(dot_prod_matrix_bots.todense(), vmin=0, vmax=0.2)\n",
    "plt.rcParams[\"figure.figsize\"] = [7.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.title('Dot-Product between bot and bot tweets', fontdict={'fontsize': 10,\n",
    " 'fontweight' : plt.rcParams['axes.titleweight'],\n",
    " 'verticalalignment': 'baseline'})\n",
    "plt.xlabel('Bot')\n",
    "plt.ylabel('Bot') \n",
    "im = plt.imshow(dot_prod_matrix_bots.todense(), cmap=\"copper_r\", vmin=0, vmax = 0.05)\n",
    "plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot Product between Human and Human Tweets\n",
    "dot_prod_matrix_human = human_tweets_tfidf * csr_matrix.transpose(human_tweets_tfidf)\n",
    "# sns.heatmap(dot_prod_matrix_human.todense(), vmin=0, vmax=0.2)\n",
    "plt.rcParams[\"figure.figsize\"] = [7.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "im = plt.imshow(dot_prod_matrix_human.todense(), cmap=\"copper_r\", vmin=0, vmax = 0.05)\n",
    "plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_prod_matrix_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of zero values in human-human dot product matrix: 0.9009316716871573\n",
      "% of zero values in bot-bot dot product matrix: 0.878362487240114\n",
      "% of zero values in human-bot dot product matrix: 0.7641726045849813\n"
     ]
    }
   ],
   "source": [
    "print(\"% of zero values in human-human dot product matrix:\", (11636*11634 - 13411199) /(11636*11634))\n",
    "print(\"% of zero values in bot-bot dot product matrix:\", (11634*11634 - 16463632) /(11634*11634))\n",
    "print(\"% of zero values in human-bot dot product matrix:\", (11636*11636 - 31930203) /(11636*11636))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
